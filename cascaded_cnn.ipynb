{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":12130948,"sourceType":"datasetVersion","datasetId":7639175},{"sourceId":12151053,"sourceType":"datasetVersion","datasetId":7652646}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport scipy.io\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import gaussian_filter\nimport random\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrowdDataset(Dataset):\n    def __init__(self, image_paths, ground_truth_paths, transform=None, augment=False, patch_size=256):\n        self.image_paths = image_paths\n        self.ground_truth_paths = ground_truth_paths\n        self.transform = transform\n        self.augment = augment\n        self.patch_size = patch_size\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        gt_path = self.ground_truth_paths[idx]\n        try:\n            mat_data = scipy.io.loadmat(gt_path)\n            points = mat_data['image_info'][0, 0][0, 0][0]\n            if points.size > 0 and len(points.shape) == 2 and points.shape[1] == 2:\n                points = points.tolist()\n            else:\n                points = []\n        except Exception as e:\n            print(f\"Error loading ground truth file {gt_path}: {e}\")\n            points = []\n        \n        density_map = self.generate_density_map(img.shape[:2], points)\n        \n        if self.augment:\n            img, density_map = self.apply_augmentation(img, density_map)\n        \n        img = cv2.resize(img, (self.patch_size, self.patch_size))\n        density_map = cv2.resize(density_map, (self.patch_size, self.patch_size))\n        \n        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n        density_map = torch.from_numpy(density_map).float().unsqueeze(0)\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        return img, density_map\n    \n    def generate_density_map(self, img_shape, points):\n        h, w = img_shape\n        density_map = np.zeros((h, w), dtype=np.float32)\n        \n        if len(points) == 0:\n            return density_map\n        \n        points = np.array(points)\n        \n        for i, point in enumerate(points):\n            x, y = int(point[0]), int(point[1])\n            \n            if x < 0 or x >= w or y < 0 or y >= h:\n                continue\n            \n            if len(points) > 1:\n                distances = np.sqrt(np.sum((points - point) ** 2, axis=1))\n                distances = np.sort(distances)[1:]  # Exclude self (distance=0)\n                \n                k = min(3, len(distances))\n                avg_distance = np.mean(distances[:k])\n                \n                sigma = max(1.0, avg_distance / 3.0)\n            else:\n                sigma = 4.0  \n            \n            kernel_size = int(6 * sigma)\n            if kernel_size % 2 == 0:\n                kernel_size += 1\n            \n            gaussian = self.gaussian_2d(kernel_size, sigma)\n            \n            x_start = max(0, x - kernel_size // 2)\n            x_end = min(w, x + kernel_size // 2 + 1)\n            y_start = max(0, y - kernel_size // 2)\n            y_end = min(h, y + kernel_size // 2 + 1)\n            \n            g_x_start = max(0, kernel_size // 2 - x)\n            g_x_end = g_x_start + (x_end - x_start)\n            g_y_start = max(0, kernel_size // 2 - y)\n            g_y_end = g_y_start + (y_end - y_start)\n            \n            density_map[y_start:y_end, x_start:x_end] += gaussian[g_y_start:g_y_end, g_x_start:g_x_end]\n        \n        return density_map\n    \n    def gaussian_2d(self, kernel_size, sigma):\n        kernel = np.zeros((kernel_size, kernel_size))\n        center = kernel_size // 2\n        \n        for i in range(kernel_size):\n            for j in range(kernel_size):\n                x, y = i - center, j - center\n                kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n        \n        return kernel / np.sum(kernel)\n    \n    def apply_augmentation(self, img, density_map):\n        if random.random() > 0.5:\n            img = cv2.flip(img, 1)\n            density_map = cv2.flip(density_map, 1)\n        \n        if random.random() > 0.5:\n            brightness_factor = random.uniform(0.8, 1.2)\n            img = np.clip(img * brightness_factor, 0, 255).astype(np.uint8)\n        \n        if random.random() > 0.7:\n            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n        \n        return img, density_map","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PReLU(nn.Module):\n    def __init__(self, num_parameters=1):\n        super(PReLU, self).__init__()\n        self.num_parameters = num_parameters\n        self.weight = nn.Parameter(torch.Tensor(num_parameters).fill_(0.25))\n    \n    def forward(self, x):\n        return F.prelu(x, self.weight)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CascadedCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CascadedCNN, self).__init__()\n\n        self.shared_conv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=9, padding=4),\n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.Conv2d(32, 64, kernel_size=7, padding=3),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout2d(0.25), \n\n            # Second block\n            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout2d(0.25), \n        )\n\n        # High-level prior branch (crowd classification) \n        self.high_level_conv = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512), \n            PReLU(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512), \n            PReLU(512),\n            nn.MaxPool2d(2, stride=2),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512), \n            PReLU(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512), \n            PReLU(512),\n        )\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 4 * 4, 1024),\n            PReLU(),\n            nn.Dropout(0.5), \n            nn.Linear(1024, 512),\n            PReLU(),\n            nn.Dropout(0.5), \n            nn.Linear(512, num_classes),\n            nn.Sigmoid()\n        )\n\n        self.density_conv = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256), \n            PReLU(256),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.Dropout2d(0.25), \n\n            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(16), \n            PReLU(16),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(16), \n            PReLU(16),\n        )\n\n        # Fusion layers \n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(16 + 512, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128), \n            PReLU(128),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            PReLU(64),\n        )\n\n        # Upsampling layers \n        self.upsample = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32), \n            PReLU(32),\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), \n            nn.BatchNorm2d(16), \n            PReLU(16),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(16), \n            PReLU(16),\n            nn.Conv2d(16, 1, kernel_size=1), \n        )\n\n    def forward(self, x):\n        shared_features = self.shared_conv(x)\n\n        high_level_features = self.high_level_conv(shared_features)\n\n        pooled_features = self.adaptive_pool(high_level_features)\n        flattened = pooled_features.view(pooled_features.size(0), -1)\n        classification = self.classifier(flattened)\n\n        density_features = self.density_conv(shared_features)\n\n        interpolated_high_level_features = F.interpolate(\n            high_level_features,\n            size=density_features.shape[2:],\n            mode='bilinear',\n            align_corners=False\n        )\n\n        fused_features = torch.cat([density_features, interpolated_high_level_features], dim=1)\n        fused_features = self.fusion_conv(fused_features)\n\n        density_map = self.upsample(fused_features)\n\n        return density_map, classification","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CrowdCountingTrainer:\n    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.density_criterion = nn.MSELoss()\n        self.classification_criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.5)\n\n        # Training history\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n        self.val_accuracies = [] # Added for classification accuracy\n\n    def get_count_class(self, count):\n        \"\"\"Convert count to classification class (0-9)\"\"\"\n        if count < 10:\n            return 0\n        elif count < 25:\n            return 1\n        elif count < 50:\n            return 2\n        elif count < 100:\n            return 3\n        elif count < 200:\n            return 4\n        elif count < 300:\n            return 5\n        elif count < 500:\n            return 6\n        elif count < 750:\n            return 7\n        elif count < 1000:\n            return 8\n        else:\n            return 9\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        total_loss = 0\n        total_mae = 0\n\n        for batch_idx, (images, density_maps) in enumerate(tqdm(train_loader, desc=\"Training\")):\n            images = images.to(self.device)\n            density_maps = density_maps.to(self.device)\n\n            # Get ground truth counts and classification labels\n            gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n            class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                      dtype=torch.long).to(self.device)\n\n            self.optimizer.zero_grad()\n\n            # Forward pass\n            pred_density, pred_class = self.model(images)\n            pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n            # Calculate losses\n            density_loss = self.density_criterion(pred_density, density_maps)\n            class_loss = self.classification_criterion(pred_class, class_labels)\n\n            # Combined loss\n            total_loss_batch = density_loss + 0.1 * class_loss\n\n            # Backward pass\n            total_loss_batch.backward()\n            self.optimizer.step()\n\n            # Calculate MAE\n            mae = torch.mean(torch.abs(pred_counts - gt_counts))\n\n            total_loss += total_loss_batch.item()\n            total_mae += mae.item()\n\n        avg_loss = total_loss / len(train_loader)\n        avg_mae = total_mae / len(train_loader)\n\n        return avg_loss, avg_mae\n\n    def validate_epoch(self, val_loader):\n        self.model.eval()\n        total_loss = 0\n        total_mae = 0\n        correct_predictions = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for images, density_maps in tqdm(val_loader, desc=\"Validation\"):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n\n                # Get ground truth counts and classification labels\n                gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                          dtype=torch.long).to(self.device)\n\n                # Forward pass\n                pred_density, pred_class = self.model(images)\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Calculate losses\n                density_loss = self.density_criterion(pred_density, density_maps)\n                class_loss = self.classification_criterion(pred_class, class_labels)\n                total_loss_batch = density_loss + 0.1 * class_loss\n\n                # Calculate MAE\n                mae = torch.mean(torch.abs(pred_counts - gt_counts))\n\n                total_loss += total_loss_batch.item()\n                total_mae += mae.item()\n\n                # Calculate accuracy for classification\n                predicted_classes = pred_class.argmax(dim=1)\n                correct_predictions += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n\n        avg_loss = total_loss / len(val_loader)\n        avg_mae = total_mae / len(val_loader)\n        accuracy = correct_predictions / total_samples # Calculate accuracy\n\n        return avg_loss, avg_mae, accuracy\n\n    def train(self, train_loader, val_loader, epochs=100, save_path='best_model.pth'):\n        best_val_mae = float('inf')\n\n        for epoch in range(epochs):\n            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n            # Training\n            train_loss, train_mae = self.train_epoch(train_loader)\n\n            # Validation\n            val_loss, val_mae, val_accuracy = self.validate_epoch(val_loader) # Get accuracy from validation\n\n            # Update learning rate\n            self.scheduler.step()\n\n            # Save best model\n            if val_mae < best_val_mae:\n                best_val_mae = val_mae\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_mae': val_mae,\n                    'val_accuracy': val_accuracy # Save validation accuracy\n                }, save_path)\n                print(f\"New best model saved with MAE: {val_mae:.4f} and Accuracy: {val_accuracy:.4f}\") # Report accuracy\n\n            # Store history\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_maes.append(train_mae)\n            self.val_maes.append(val_mae)\n            self.val_accuracies.append(val_accuracy) # Store validation accuracy\n\n            print(f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val Accuracy: {val_accuracy:.4f}\") # Report accuracy\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n\n\n    def test(self, test_loader, model_path='best_model.pth'):\n        \"\"\"Test the model on test dataset\"\"\"\n        # Load best model\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n        self.model.eval()\n        total_mae = 0\n        total_mse = 0\n        correct_predictions = 0\n        total_samples = 0\n        all_predictions = []\n        all_ground_truths = []\n\n\n        with torch.no_grad():\n            for images, density_maps in tqdm(test_loader, desc=\"Testing\"):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n\n                # Get ground truth counts and classification labels\n                gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                          dtype=torch.long).to(self.device)\n\n\n                # Forward pass\n                pred_density, pred_class = self.model(images) # Get classification predictions\n\n                # Calculate counts\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Store predictions\n                all_predictions.extend(pred_counts.cpu().numpy())\n                all_ground_truths.extend(gt_counts.cpu().numpy())\n\n\n                # Calculate errors\n                mae = torch.mean(torch.abs(pred_counts - gt_counts))\n                mse = torch.mean((pred_counts - gt_counts) ** 2)\n\n                total_mae += mae.item()\n                total_mse += mse.item()\n\n                # Calculate accuracy for classification\n                predicted_classes = pred_class.argmax(dim=1)\n                correct_predictions += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n        avg_mae = total_mae / len(test_loader)\n        avg_mse = total_mse / len(test_loader)\n        test_accuracy = correct_predictions / total_samples # Calculate test accuracy\n\n        print(f\"\\nTest Results:\")\n        print(f\"MAE: {avg_mae:.4f}\")\n        print(f\"MSE: {avg_mse:.4f}\")\n        print(f\"RMSE: {np.sqrt(avg_mse):.4f}\")\n        print(f\"Accuracy: {test_accuracy:.4f}\") # Report test accuracy\n\n        return avg_mae, avg_mse, test_accuracy, all_predictions, all_ground_truths # Return test accuracy\n\n    def plot_training_history(self):\n        \"\"\"Plot training history\"\"\"\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5)) # Increased figure size\n\n        # Plot losses\n        ax1.plot(self.train_losses, label='Train Loss')\n        ax1.plot(self.val_losses, label='Validation Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot MAE\n        ax2.plot(self.train_maes, label='Train MAE')\n        ax2.plot(self.val_maes, label='Validation MAE')\n        ax2.set_title('Training and Validation MAE')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('MAE')\n        ax2.legend()\n        ax2.grid(True)\n\n        # Plot Accuracy\n        ax3.plot(self.val_accuracies, label='Validation Accuracy', color='green') # Plot validation accuracy\n        ax3.set_title('Validation Accuracy')\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Accuracy')\n        ax3.legend()\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data_paths(train_images_dir, train_gt_dir, test_images_dir, test_gt_dir):\n    \n    train_img_paths = []\n    train_gt_paths = []\n    \n    for img_file in os.listdir(train_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(train_images_dir, img_file)\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(train_gt_dir, gt_file)\n            \n            if os.path.exists(gt_path):\n                train_img_paths.append(img_path)\n                train_gt_paths.append(gt_path)\n    \n    test_img_paths = []\n    test_gt_paths = []\n    \n    for img_file in os.listdir(test_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(test_images_dir, img_file)\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(test_gt_dir, gt_file)\n            \n            if os.path.exists(gt_path):\n                test_img_paths.append(img_path)\n                test_gt_paths.append(gt_path)\n    \n    return train_img_paths, train_gt_paths, test_img_paths, test_gt_paths","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \n    # Configuration\n    config = {\n        'train_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/images',  # UPDATE THIS PATH\n        'train_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/ground_truth',  # UPDATE THIS PATH\n        'test_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/images',  # UPDATE THIS PATH\n        'test_gt_dir': '/kaggle/input/shanghai-tech/part_A_final/test_data/ground_truth',  # UPDATE THIS PATH\n        'batch_size': 16,\n        'epochs': 20,\n        'patch_size': 256,\n        'val_split': 0.2,\n        'num_workers': 4,\n        'save_path': 'cascaded_cnn_best.pth'\n    }\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    print(\"Loading data paths...\")\n    train_img_paths, train_gt_paths, test_img_paths, test_gt_paths = load_data_paths(\n        config['train_images_dir'], \n        config['train_gt_dir'],\n        config['test_images_dir'], \n        config['test_gt_dir']\n    )\n    \n    print(f\"Found {len(train_img_paths)} training images\")\n    print(f\"Found {len(test_img_paths)} test images\")\n    \n    train_imgs, val_imgs, train_gts, val_gts = train_test_split(\n        train_img_paths, train_gt_paths, \n        test_size=config['val_split'], \n        random_state=42\n    )\n    \n    print(f\"Train set: {len(train_imgs)} images\")\n    print(f\"Validation set: {len(val_imgs)} images\")\n    \n    train_dataset = CrowdDataset(\n        train_imgs, train_gts, \n        augment=True, \n        patch_size=config['patch_size']\n    )\n    \n    val_dataset = CrowdDataset(\n        val_imgs, val_gts, \n        augment=False, \n        patch_size=config['patch_size']\n    )\n    \n    test_dataset = CrowdDataset(\n        test_img_paths, test_gt_paths, \n        augment=False, \n        patch_size=config['patch_size']\n    )\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config['batch_size'], \n        shuffle=True, \n        num_workers=config['num_workers']\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config['batch_size'], \n        shuffle=False, \n        num_workers=config['num_workers']\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=config['batch_size'], \n        shuffle=False, \n        num_workers=config['num_workers']\n    )\n    \n    model = CascadedCNN(num_classes=10)\n    model.to(device)\n    trainer = CrowdCountingTrainer(model)\n    \n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n    \n    print(\"Starting training...\")\n    trainer.train(\n        train_loader, \n        val_loader, \n        epochs=config['epochs'], \n        save_path=config['save_path']\n    )\n    \n    trainer.plot_training_history()\n    \n    print(\"Testing model...\")\n    test_mae, test_mse, test_accuracy, predictions, ground_truths = trainer.test( # Get test accuracy\n        test_loader,\n        model_path=config['save_path']\n    )\n    \n    plt.figure(figsize=(12, 4))\n    indices = np.random.choice(len(predictions), 20, replace=False)\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(np.array(ground_truths)[indices], np.array(predictions)[indices], alpha=0.6)\n    plt.plot([0, max(ground_truths)], [0, max(ground_truths)], 'r--', lw=2)\n    plt.xlabel('Ground Truth Count')\n    plt.ylabel('Predicted Count')\n    plt.title('Predicted vs Ground Truth')\n    plt.grid(True)\n    \n    plt.subplot(1, 2, 2)\n    errors = np.array(predictions) - np.array(ground_truths)\n    plt.hist(errors, bins=30, alpha=0.7)\n    plt.xlabel('Prediction Error')\n    plt.ylabel('Frequency')\n    plt.title('Error Distribution')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"Training and testing completed!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-15T06:58:57.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython import get_ipython\nfrom IPython.display import display\n# %%\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport scipy.io\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import gaussian_filter\nimport random\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n# %%\nclass CrowdDataset(Dataset):\n    def __init__(self, image_paths, ground_truth_paths, transform=None, augment=False, patch_size=256):\n        self.image_paths = image_paths\n        self.ground_truth_paths = ground_truth_paths\n        self.transform = transform\n        self.augment = augment\n        self.patch_size = patch_size\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Load ground truth from .mat file\n        gt_path = self.ground_truth_paths[idx]\n        try:\n            mat_data = scipy.io.loadmat(gt_path)\n            # Extract point coordinates from image_info[0,0][0,0][0]\n            points = mat_data['image_info'][0, 0][0, 0][0]\n            # Convert to list of [x, y] coordinates if it's a valid array\n            if points.size > 0 and len(points.shape) == 2 and points.shape[1] == 2:\n                points = points.tolist()\n            else:\n                points = []\n        except Exception as e:\n            print(f\"Error loading ground truth file {gt_path}: {e}\")\n            points = []\n\n        # Generate density map\n        density_map = self.generate_density_map(img.shape[:2], points)\n\n        # Apply augmentation if specified\n        if self.augment:\n            img, density_map = self.apply_augmentation(img, density_map)\n\n        # Resize to patch size\n        img = cv2.resize(img, (self.patch_size, self.patch_size))\n        density_map = cv2.resize(density_map, (self.patch_size, self.patch_size))\n\n        # Convert to tensor\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n        density_map = torch.from_numpy(density_map).float().unsqueeze(0)\n\n        # Apply transforms if any\n        if self.transform:\n            img = self.transform(img)\n\n        return img, density_map\n\n    def generate_density_map(self, img_shape, points):\n        \"\"\"Generate density map from point annotations using adaptive Gaussian kernels\"\"\"\n        h, w = img_shape\n        density_map = np.zeros((h, w), dtype=np.float32)\n\n        if len(points) == 0:\n            return density_map\n\n        # Convert points to numpy array\n        points = np.array(points)\n\n        # For each point, calculate adaptive sigma based on k-nearest neighbors\n        for i, point in enumerate(points):\n            x, y = int(point[0]), int(point[1])\n\n            # Skip if point is outside image bounds\n            if x < 0 or x >= w or y < 0 or y >= h:\n                continue\n\n            # Calculate distance to k nearest neighbors (k=3)\n            if len(points) > 1:\n                distances = np.sqrt(np.sum((points - point) ** 2, axis=1))\n                distances = np.sort(distances)[1:]  # Exclude self (distance=0)\n\n                # Use average of 3 nearest neighbors or all if less than 3\n                k = min(3, len(distances))\n                avg_distance = np.mean(distances[:k])\n\n                # Adaptive sigma based on local density\n                sigma = max(1.0, avg_distance / 3.0)\n            else:\n                sigma = 4.0  # Default sigma for single point\n\n            # Create Gaussian kernel\n            kernel_size = int(6 * sigma)\n            if kernel_size % 2 == 0:\n                kernel_size += 1\n\n            # Generate 2D Gaussian\n            gaussian = self.gaussian_2d(kernel_size, sigma)\n\n            # Add to density map\n            x_start = max(0, x - kernel_size // 2)\n            x_end = min(w, x + kernel_size // 2 + 1)\n            y_start = max(0, y - kernel_size // 2)\n            y_end = min(h, y + kernel_size // 2 + 1)\n\n            # Adjust Gaussian bounds\n            g_x_start = max(0, kernel_size // 2 - x)\n            g_x_end = g_x_start + (x_end - x_start)\n            g_y_start = max(0, kernel_size // 2 - y)\n            g_y_end = g_y_start + (y_end - y_start)\n\n            density_map[y_start:y_end, x_start:x_end] += gaussian[g_y_start:g_y_end, g_x_start:g_x_end]\n\n        return density_map\n\n    def gaussian_2d(self, kernel_size, sigma):\n        \"\"\"Generate 2D Gaussian kernel\"\"\"\n        kernel = np.zeros((kernel_size, kernel_size))\n        center = kernel_size // 2\n\n        for i in range(kernel_size):\n            for j in range(kernel_size):\n                x, y = i - center, j - center\n                kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n\n        return kernel / np.sum(kernel)\n\n    def apply_augmentation(self, img, density_map):\n        \"\"\"Apply data augmentation\"\"\"\n        # Random horizontal flip\n        if random.random() > 0.5:\n            img = cv2.flip(img, 1)\n            density_map = cv2.flip(density_map, 1)\n\n        # Random brightness adjustment\n        if random.random() > 0.5:\n            brightness_factor = random.uniform(0.8, 1.2)\n            img = np.clip(img * brightness_factor, 0, 255).astype(np.uint8)\n\n        # Random noise\n        if random.random() > 0.7:\n            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n\n        return img, density_map\n# %%\nclass PReLU(nn.Module):\n    \"\"\"Parametric ReLU activation function\"\"\"\n    def __init__(self, num_parameters=1):\n        super(PReLU, self).__init__()\n        self.num_parameters = num_parameters\n        self.weight = nn.Parameter(torch.Tensor(num_parameters).fill_(0.25))\n\n    def forward(self, x):\n        return F.prelu(x, self.weight)\n\n# %%\n# Modify the CascadedCNN class\n\nclass CascadedCNN(nn.Module):\n    \"\"\"Simplified Cascaded CNN for Crowd Density Estimation (Using Tanh)\"\"\"\n    def __init__(self, num_classes=5): # Updated num_classes to 5\n        super(CascadedCNN, self).__init__()\n\n        # Shared front-end network (simplified convolutional layers)\n        self.shared_conv = nn.Sequential(\n            # First block\n            nn.Conv2d(3, 32, kernel_size=5, padding=2), # Smaller kernel\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Use Tanh instead of ReLU/PReLU\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Smaller kernel\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Use Tanh\n            nn.MaxPool2d(2, stride=2),\n\n            # Second block\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(), # Use Tanh\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        # High-level prior branch (simplified convolutional layers)\n        self.high_level_conv = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.Tanh(), # Use Tanh\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.Tanh(), # Use Tanh\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        # Adaptive pooling for variable input sizes\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n\n        # Fully connected layers for classification (simplified)\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 4 * 4, 512),\n            nn.Tanh(), # Use Tanh\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes),\n            # Keep Sigmoid if you are using it for the last layer of the classifier\n            # based on your problem (e.g., multi-label classification)\n            # If it's multi-class classification with CrossEntropyLoss,\n            # you typically don't need Sigmoid here (the loss function handles it).\n            # Let's assume multi-class and remove Sigmoid for now,\n            # as you are using CrossEntropyLoss.\n            # nn.Sigmoid()\n        )\n\n        # Density estimation branch (simplified convolutional layers)\n        self.density_conv = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(), # Use Tanh\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Use Tanh\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Use Tanh\n        )\n\n        # Fusion layers (simplified)\n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(32 + 256, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Use Tanh\n        )\n\n        # Upsampling layers (simplified)\n        self.upsample = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Use Tanh\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(16),\n            nn.Tanh(), # Use Tanh\n            nn.Conv2d(16, 1, kernel_size=1), # Final 1x1 convolution\n        )\n\n\n    def forward(self, x):\n        # Shared front-end\n        shared_features = self.shared_conv(x)\n\n        # High-level prior branch\n        high_level_features = self.high_level_conv(shared_features)\n\n        # Classification head\n        pooled_features = self.adaptive_pool(high_level_features)\n        flattened = pooled_features.view(pooled_features.size(0), -1)\n        classification = self.classifier(flattened) # Output logits for CrossEntropyLoss\n\n        # Density estimation branch\n        density_features = self.density_conv(shared_features)\n\n        # Fusion\n        interpolated_high_level_features = F.interpolate(\n            high_level_features,\n            size=density_features.shape[2:],\n            mode='bilinear',\n            align_corners=False\n        )\n\n        fused_features = torch.cat([density_features, interpolated_high_level_features], dim=1)\n        fused_features = self.fusion_conv(fused_features)\n\n        # Upsampling\n        density_map = self.upsample(fused_features)\n\n        return density_map, classification\n        \nclass CrowdCountingTrainer:\n    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.density_criterion = nn.MSELoss()\n        # Use CrossEntropyLoss with logits (no Sigmoid in the model's last layer)\n        self.classification_criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.5)\n\n        # Training history\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n        self.val_accuracies_classification = []\n        # Removed val_accuracies_ratio\n        self.train_accuracies_classification = []\n\n\n    def get_count_class(self, count):\n        \"\"\"Convert count to classification class (5 classes up to 100)\"\"\"\n        if count < 10:\n            return 0\n        elif count < 30:\n            return 1\n        elif count < 60:\n            return 2\n        elif count < 100:\n            return 3\n        else:\n            return 4 # Counts >= 100\n\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        total_loss = 0\n        total_mae = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n\n        for batch_idx, (images, density_maps) in enumerate(tqdm(train_loader, desc=\"Training\")):\n            images = images.to(self.device)\n            density_maps = density_maps.to(self.device)\n\n            # Get ground truth counts and classification labels\n            gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n            class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                      dtype=torch.long).to(self.device)\n\n            self.optimizer.zero_grad()\n\n            # Forward pass\n            pred_density, pred_class_logits = self.model(images) # Get logits\n            pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n            # Calculate losses\n            density_loss = self.density_criterion(pred_density, density_maps)\n            # Use CrossEntropyLoss with logits\n            class_loss = self.classification_criterion(pred_class_logits, class_labels)\n\n\n            # Combined loss\n            classification_weight = 0.1 # You can adjust this weight\n            total_loss_batch = density_loss + classification_weight * class_loss\n\n\n            # Backward pass\n            total_loss_batch.backward()\n            self.optimizer.step()\n\n            # Calculate MAE\n            mae = torch.mean(torch.abs(pred_counts - gt_counts))\n\n            total_loss += total_loss_batch.item()\n            total_mae += mae.item()\n\n            # Calculate classification accuracy for training\n            # Use argmax on logits to get predicted classes\n            predicted_classes = pred_class_logits.argmax(dim=1)\n            correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n            total_samples += class_labels.size(0)\n\n\n        avg_loss = total_loss / len(train_loader)\n        avg_mae = total_mae / len(train_loader)\n        train_accuracy_classification = correct_predictions_classification / total_samples\n\n        return avg_loss, avg_mae, train_accuracy_classification\n\n\n    def validate_epoch(self, val_loader):\n        self.model.eval()\n        total_loss = 0\n        total_mae = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for images, density_maps in tqdm(val_loader, desc=\"Validation\"):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n\n                # Get ground truth counts and classification labels\n                gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                          dtype=torch.long).to(self.device)\n\n                # Forward pass\n                pred_density, pred_class_logits = self.model(images) # Get logits\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Calculate losses\n                density_loss = self.density_criterion(pred_density, density_maps)\n                # Use CrossEntropyLoss with logits\n                class_loss = self.classification_criterion(pred_class_logits, class_labels)\n                classification_weight = 0.1 # Match weight from training\n                total_loss_batch = density_loss + classification_weight * class_loss\n\n                # Calculate MAE\n                mae = torch.mean(torch.abs(pred_counts - gt_counts))\n\n                total_loss += total_loss_batch.item()\n                total_mae += mae.item()\n\n                # Calculate classification accuracy\n                # Use argmax on logits to get predicted classes\n                predicted_classes = pred_class_logits.argmax(dim=1)\n                correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n                # Print ground truth and predicted classes for a few batches\n                print(f\"Ground Truth Classes: {class_labels.cpu().numpy()}\")\n                print(f\"Predicted Classes: {predicted_classes.cpu().numpy()}\")\n\n\n        avg_loss = total_loss / len(val_loader)\n        avg_mae = total_mae / len(val_loader)\n        accuracy_classification = correct_predictions_classification / total_samples\n\n        return avg_loss, avg_mae, accuracy_classification\n\n    def train(self, train_loader, val_loader, epochs=100, save_path='best_model.pth'):\n        best_val_mae = float('inf')\n\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n        self.val_accuracies_classification = []\n        # Removed val_accuracies_ratio\n        self.train_accuracies_classification = []\n\n\n        for epoch in range(epochs):\n            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n            # Training\n            train_loss, train_mae, train_accuracy_classification = self.train_epoch(train_loader)\n\n            # Validation\n            val_loss, val_mae, val_accuracy_classification = self.validate_epoch(val_loader)\n\n            # Update learning rate\n            self.scheduler.step()\n\n            # Save best model (you might want to save based on MAE as before)\n            if val_mae < best_val_mae:\n                best_val_mae = val_mae\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_mae': val_mae,\n                    'val_accuracy_classification': val_accuracy_classification,\n                    # Removed val_accuracy_ratio from save\n                }, save_path)\n                print(f\"New best model saved with MAE: {val_mae:.4f}, Classification Accuracy: {val_accuracy_classification:.4f}\")\n\n\n            # Store history\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_maes.append(train_mae)\n            self.val_maes.append(val_mae)\n            self.val_accuracies_classification.append(val_accuracy_classification)\n            # Removed appending to val_accuracies_ratio\n            self.train_accuracies_classification.append(train_accuracy_classification)\n\n\n            print(f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train Classification Accuracy: {train_accuracy_classification:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val Classification Accuracy: {val_accuracy_classification:.4f}\")\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n\n\n    def test(self, test_loader, model_path='best_model.pth'):\n        \"\"\"Test the model on test dataset\"\"\"\n        # Load best model\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n        self.model.eval()\n        total_mae = 0\n        total_mse = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n        all_predictions = []\n        all_ground_truths = []\n\n\n        with torch.no_grad():\n            for images, density_maps in tqdm(test_loader, desc=\"Testing\"):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n\n                # Get ground truth counts and classification labels\n                gt_counts = torch.sum(density_maps, dim=(1, 2, 3))\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in gt_counts],\n                                          dtype=torch.long).to(self.device)\n\n\n                # Forward pass\n                pred_density, pred_class_logits = self.model(images) # Get logits\n\n                # Calculate counts\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Store predictions\n                all_predictions.extend(pred_counts.cpu().numpy())\n                all_ground_truths.extend(gt_counts.cpu().numpy())\n\n\n                # Calculate errors\n                mae = torch.mean(torch.abs(pred_counts - gt_counts))\n                mse = torch.mean((pred_counts - gt_counts) ** 2)\n\n                total_mae += mae.item()\n                total_mse += mse.item()\n\n                # Calculate classification accuracy\n                # Use argmax on logits to get predicted classes\n                predicted_classes = pred_class_logits.argmax(dim=1)\n                correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n\n        avg_mae = total_mae / len(test_loader)\n        avg_mse = total_mse / len(test_loader)\n        test_accuracy_classification = correct_predictions_classification / total_samples\n\n\n        print(f\"\\nTest Results:\")\n        print(f\"MAE: {avg_mae:.4f}\")\n        print(f\"MSE: {avg_mse:.4f}\")\n        print(f\"RMSE: {np.sqrt(avg_mse):.4f}\")\n        print(f\"Classification Accuracy: {test_accuracy_classification:.4f}\")\n\n\n        return avg_mae, avg_mse, test_accuracy_classification, all_predictions, all_ground_truths\n\n    def plot_training_history(self):\n        \"\"\"Plot training history\"\"\"\n        # Changed to 3 subplots as we are not plotting ratio accuracy\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot losses\n        ax1.plot(self.train_losses, label='Train Loss')\n        ax1.plot(self.val_losses, label='Validation Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot MAE\n        ax2.plot(self.train_maes, label='Train MAE')\n        ax2.plot(self.val_maes, label='Validation MAE')\n        ax2.set_title('Training and Validation MAE')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('MAE')\n        ax2.legend()\n        ax2.grid(True)\n\n        # Plot Classification Accuracy\n        ax3.plot(self.train_accuracies_classification, label='Train Classification Accuracy', color='red')\n        ax3.plot(self.val_accuracies_classification, label='Validation Classification Accuracy', color='green')\n        ax3.set_title('Training and Validation Classification Accuracy')\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Accuracy')\n        ax3.legend()\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n# %%\ndef load_data_paths(train_images_dir, train_gt_dir, test_images_dir, test_gt_dir):\n    \"\"\"Load image and ground truth file paths\"\"\"\n\n    # Get training data paths\n    train_img_paths = []\n    train_gt_paths = []\n\n    for img_file in os.listdir(train_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(train_images_dir, img_file)\n            # Assuming ground truth files have same name but with .mat extension\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(train_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                train_img_paths.append(img_path)\n                train_gt_paths.append(gt_path)\n\n    # Get test data paths\n    test_img_paths = []\n    test_gt_paths = []\n\n    for img_file in os.listdir(test_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(test_images_dir, img_file)\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(test_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                test_img_paths.append(img_path)\n                test_gt_paths.append(gt_path)\n\n    return train_img_paths, train_gt_paths, test_img_paths, test_gt_paths\n# %%\ndef main():\n    \"\"\"Main function to run the complete pipeline\"\"\"\n\n    # Configuration\n    config = {\n        'train_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/images',  # UPDATE THIS PATH\n        'train_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/ground_truth',  # UPDATE THIS PATH\n        'test_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/images',  # UPDATE THIS PATH\n        'test_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/ground_truth',  # UPDATE THIS PATH\n        'batch_size': 16,\n        'epochs': 30,\n        'patch_size': 256,\n        'val_split': 0.2,\n        'num_workers': 4,\n        'save_path': 'cascaded_cnn_best.pth',\n        'num_classes': 5 # Updated number of classes to 5\n    }\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    print(\"Loading data paths...\")\n    train_img_paths, train_gt_paths, test_img_paths, test_gt_paths = load_data_paths(\n        config['train_images_dir'],\n        config['train_gt_dir'],\n        config['test_images_dir'],\n        config['test_gt_dir']\n    )\n\n    print(f\"Found {len(train_img_paths)} training images\")\n    print(f\"Found {len(test_img_paths)} test images\")\n\n    # Split training data into train and validation\n    train_imgs, val_imgs, train_gts, val_gts = train_test_split(\n        train_img_paths, train_gt_paths,\n        test_size=config['val_split'],\n        random_state=42\n    )\n\n    print(f\"Train set: {len(train_imgs)} images\")\n    print(f\"Validation set: {len(val_imgs)} images\")\n\n    # Create datasets\n    train_dataset = CrowdDataset(\n        train_imgs, train_gts,\n        augment=True,\n        patch_size=config['patch_size']\n    )\n\n    val_dataset = CrowdDataset(\n        val_imgs, val_gts,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    test_dataset = CrowdDataset(\n        test_img_paths, test_gt_paths,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers']\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    # Initialize model and trainer\n    model = CascadedCNN(num_classes=config['num_classes']) # Pass num_classes to model\n    model.to(device)\n    trainer = CrowdCountingTrainer(model)\n\n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n\n    # Train the model\n    print(\"Starting training...\")\n    trainer.train(\n        train_loader,\n        val_loader,\n        epochs=config['epochs'],\n        save_path=config['save_path']\n    )\n\n    # Plot training history\n    trainer.plot_training_history()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:46:05.314026Z","iopub.execute_input":"2025-06-16T05:46:05.314341Z","iopub.status.idle":"2025-06-16T05:46:12.002170Z","shell.execute_reply.started":"2025-06-16T05:46:05.314317Z","shell.execute_reply":"2025-06-16T05:46:12.001591Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:46:21.202157Z","iopub.execute_input":"2025-06-16T05:46:21.202710Z","iopub.status.idle":"2025-06-16T05:47:09.106648Z","shell.execute_reply.started":"2025-06-16T05:46:21.202686Z","shell.execute_reply":"2025-06-16T05:47:09.105461Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data paths...\nFound 400 training images\nFound 316 test images\nTrain set: 320 images\nValidation set: 80 images\nModel initialized with 3529222 parameters\nStarting training...\n\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/20 [00:45<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/2492607678.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     trainer.train(\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2492607678.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs, save_path)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2492607678.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity_maps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mdensity_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensity_maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"from IPython import get_ipython\nfrom IPython.display import display\n# %%\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport scipy.io\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import gaussian_filter\nimport random\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n# %%\nclass CrowdDataset(Dataset):\n    def __init__(self, image_paths, ground_truth_paths, transform=None, augment=False, patch_size=256):\n        self.image_paths = image_paths\n        self.ground_truth_paths = ground_truth_paths\n        self.transform = transform\n        self.augment = augment\n        self.patch_size = patch_size\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Load ground truth from .mat file\n        gt_path = self.ground_truth_paths[idx]\n        points = [] # Initialize points for density map generation\n        actual_count = 0.0 # Initialize actual_count\n\n        try:\n            mat_data = scipy.io.loadmat(gt_path)\n            # Extract point coordinates (still needed for density map)\n            extracted_points = mat_data['image_info'][0, 0][0, 0][0]\n            if extracted_points.size > 0 and len(extracted_points.shape) == 2 and extracted_points.shape[1] == 2:\n                points = extracted_points.tolist()\n            else:\n                points = []\n\n            # **New Logic: Extract actual count directly**\n            # Ensure the count data is numeric and can be converted to a float\n            raw_count = mat_data['image_info'][0, 0][0, 0][1]\n            actual_count = float(raw_count) # Convert to float\n\n        except Exception as e:\n            print(f\"Error loading ground truth file {gt_path}: {e}\")\n            points = []\n            actual_count = 0.0 # Default to 0 if count cannot be loaded\n\n        # Generate density map from points (as before)\n        density_map = self.generate_density_map(img.shape[:2], points)\n\n        # Apply augmentation if specified\n        if self.augment:\n            img, density_map = self.apply_augmentation(img, density_map)\n\n        # Resize to patch size\n        img = cv2.resize(img, (self.patch_size, self.patch_size))\n        density_map = cv2.resize(density_map, (self.patch_size, self.patch_size))\n\n        # Convert to tensor\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n        density_map = torch.from_numpy(density_map).float().unsqueeze(0)\n\n        # Apply transforms if any\n        if self.transform:\n            img = self.transform(img)\n\n        # Return the image, density map, and the actual count\n        return img, density_map, torch.tensor(actual_count, dtype=torch.float32)\n        \n    def generate_density_map(self, img_shape, points):\n        \"\"\"Generate density map from point annotations using adaptive Gaussian kernels\"\"\"\n        h, w = img_shape\n        density_map = np.zeros((h, w), dtype=np.float32)\n\n        if len(points) == 0:\n            return density_map\n\n        # Convert points to numpy array\n        points = np.array(points)\n\n        # For each point, calculate adaptive sigma based on k-nearest neighbors\n        for i, point in enumerate(points):\n            x, y = int(point[0]), int(point[1])\n\n            # Skip if point is outside image bounds\n            if x < 0 or x >= w or y < 0 or y >= h:\n                continue\n\n            # Calculate distance to k nearest neighbors (k=3)\n            if len(points) > 1:\n                distances = np.sqrt(np.sum((points - point) ** 2, axis=1))\n                distances = np.sort(distances)[1:]  # Exclude self (distance=0)\n\n                # Use average of 3 nearest neighbors or all if less than 3\n                k = min(3, len(distances))\n                avg_distance = np.mean(distances[:k])\n\n                # Adaptive sigma based on local density\n                sigma = max(1.0, avg_distance / 3.0)\n            else:\n                sigma = 4.0  # Default sigma for single point\n\n            # Create Gaussian kernel\n            kernel_size = int(6 * sigma)\n            if kernel_size % 2 == 0:\n                kernel_size += 1\n\n            # Generate 2D Gaussian\n            gaussian = self.gaussian_2d(kernel_size, sigma)\n\n            # Add to density map\n            x_start = max(0, x - kernel_size // 2)\n            x_end = min(w, x + kernel_size // 2 + 1)\n            y_start = max(0, y - kernel_size // 2)\n            y_end = min(h, y + kernel_size // 2 + 1)\n\n            # Adjust Gaussian bounds\n            g_x_start = max(0, kernel_size // 2 - x)\n            g_x_end = g_x_start + (x_end - x_start)\n            g_y_start = max(0, kernel_size // 2 - y)\n            g_y_end = g_y_start + (y_end - y_start)\n\n            density_map[y_start:y_end, x_start:x_end] += gaussian[g_y_start:g_y_end, g_x_start:g_x_end]\n\n        return density_map\n\n    def gaussian_2d(self, kernel_size, sigma):\n        \"\"\"Generate 2D Gaussian kernel\"\"\"\n        kernel = np.zeros((kernel_size, kernel_size))\n        center = kernel_size // 2\n\n        for i in range(kernel_size):\n            for j in range(kernel_size):\n                x, y = i - center, j - center\n                kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n\n        return kernel / np.sum(kernel)\n\n    def apply_augmentation(self, img, density_map):\n        \"\"\"Apply data augmentation\"\"\"\n        # Random horizontal flip\n        if random.random() > 0.5:\n            img = cv2.flip(img, 1)\n            density_map = cv2.flip(density_map, 1)\n\n        # Random brightness adjustment\n        if random.random() > 0.5:\n            brightness_factor = random.uniform(0.8, 1.2)\n            img = np.clip(img * brightness_factor, 0, 255).astype(np.uint8)\n\n        # Random noise\n        if random.random() > 0.7:\n            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n\n        return img, density_map\n# %%\n# Removed the custom PReLU class since we are using nn.Tanh\n\n# %%\n# Modify the CascadedCNN class\n\nclass CascadedCNN(nn.Module):\n    \"\"\"Simplified Cascaded CNN for Crowd Density Estimation (Using Tanh)\"\"\"\n    def __init__(self, num_classes=4): # Updated num_classes to 4\n        super(CascadedCNN, self).__init__()\n\n        # Shared front-end network (simplified convolutional layers)\n        self.shared_conv = nn.Sequential(\n            # First block\n            nn.Conv2d(3, 32, kernel_size=5, padding=2), # Smaller kernel\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Using Tanh\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Smaller kernel\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Using Tanh\n            nn.MaxPool2d(2, stride=2),\n\n            # Second block\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(), # Using Tanh\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        # High-level prior branch (simplified convolutional layers)\n        self.high_level_conv = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.Tanh(), # Using Tanh\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.Tanh(), # Using Tanh\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        # Adaptive pooling for variable input sizes\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n\n        # Fully connected layers for classification (simplified)\n        self.classifier = nn.Sequential(\n            nn.Linear(256 * 4 * 4, 512), # Reduced input size based on high_level_conv output\n            nn.Tanh(), # Using Tanh\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes), # Reduced size, updated to num_classes\n            # nn.Sigmoid() # Using CrossEntropyLoss, no sigmoid in the last layer\n        )\n\n        # Density estimation branch (simplified convolutional layers)\n        self.density_conv = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(), # Using Tanh\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Using Tanh\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Using Tanh\n        )\n\n        # Fusion layers (simplified)\n        # Input channels: output from density_conv (32) + output from high_level_conv (256) after interpolation\n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(32 + 256, 64, kernel_size=3, padding=1), # Adjusted input channels\n            nn.BatchNorm2d(64),\n            nn.Tanh(), # Using Tanh\n        )\n\n        # Upsampling layers (simplified)\n        self.upsample = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # Upsample to 128x128\n            nn.BatchNorm2d(32),\n            nn.Tanh(), # Using Tanh\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # Upsample to 256x256\n            nn.BatchNorm2d(16),\n            nn.Tanh(), # Using Tanh\n            nn.Conv2d(16, 1, kernel_size=1), # Final 1x1 convolution\n        )\n\n\n    def forward(self, x):\n        # Shared front-end\n        shared_features = self.shared_conv(x)\n\n        # High-level prior branch\n        high_level_features = self.high_level_conv(shared_features)\n\n        # Classification head\n        pooled_features = self.adaptive_pool(high_level_features)\n        flattened = pooled_features.view(pooled_features.size(0), -1)\n        classification = self.classifier(flattened) # Output logits for CrossEntropyLoss\n\n        # Density estimation branch\n        density_features = self.density_conv(shared_features)\n\n        # Fusion\n        interpolated_high_level_features = F.interpolate(\n            high_level_features,\n            size=density_features.shape[2:],\n            mode='bilinear',\n            align_corners=False\n        )\n\n        fused_features = torch.cat([density_features, interpolated_high_level_features], dim=1)\n        fused_features = self.fusion_conv(fused_features)\n\n        # Upsampling\n        density_map = self.upsample(fused_features)\n\n        return density_map, classification # Return logits for classification\n# %%\nclass CrowdCountingTrainer:\n    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.density_criterion = nn.MSELoss()\n        # Use CrossEntropyLoss with logits (no Sigmoid in the model's last layer)\n        self.classification_criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.5)\n\n        # Training history\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n        self.val_accuracies_classification = []\n        self.train_accuracies_classification = []\n\n\n    def get_count_class(self, count):\n        \"\"\"Convert count to classification class based on new ranges\"\"\"\n        if count < 20:\n            return 0\n        elif count < 40:\n            return 1\n        elif count < 80:\n            return 2\n        elif count < 120:\n            return 3\n        elif count < 200:\n            return 4\n        else: # count >= 200\n            return 5\n\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        total_loss = 0\n        total_mae = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n\n        for batch_idx, (images, density_maps, actual_counts_from_gt) in enumerate(tqdm(train_loader, desc=\"Training\")):\n            images = images.to(self.device)\n            density_maps = density_maps.to(self.device)\n            actual_counts_from_gt = actual_counts_from_gt.to(self.device) # Move to device\n\n            # Use actual_counts_from_gt for classification labels\n            class_labels = torch.tensor([self.get_count_class(count.item()) for count in actual_counts_from_gt],\n                                        dtype=torch.long).to(self.device)\n\n            self.optimizer.zero_grad()\n\n            pred_density, pred_class_logits = self.model(images)\n            pred_counts = torch.sum(pred_density, dim=(1, 2, 3)) # This is still the predicted count from density map\n\n            density_loss = self.density_criterion(pred_density, density_maps)\n            class_loss = self.classification_criterion(pred_class_logits, class_labels)\n\n            classification_weight = 0.1\n            total_loss_batch = density_loss + classification_weight * class_loss\n\n            total_loss_batch.backward()\n            self.optimizer.step()\n\n            # For MAE, you should now use actual_counts_from_gt for ground truth\n            mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n\n            total_loss += total_loss_batch.item()\n            total_mae += mae.item()\n\n            # Calculate classification accuracy for training\n            # Use argmax on logits to get predicted classes\n            predicted_classes = pred_class_logits.argmax(dim=1)\n            correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n            total_samples += class_labels.size(0)\n\n\n        avg_loss = total_loss / len(train_loader)\n        avg_mae = total_mae / len(train_loader)\n        train_accuracy_classification = correct_predictions_classification / total_samples\n\n        return avg_loss, avg_mae, train_accuracy_classification\n\n\n    def validate_epoch(self, val_loader):\n        self.model.eval()\n        total_loss = 0\n        total_mae = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for batch_idx, (images, density_maps, actual_counts_from_gt) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n                actual_counts_from_gt = actual_counts_from_gt.to(self.device)\n\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in actual_counts_from_gt],\n                                            dtype=torch.long).to(self.device)\n\n                # Forward pass\n                pred_density, pred_class_logits = self.model(images) # Get logits\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Calculate losses\n                density_loss = self.density_criterion(pred_density, density_maps)\n                # Use CrossEntropyLoss with logits\n                class_loss = self.classification_criterion(pred_class_logits, class_labels)\n                classification_weight = 0.1 # Match weight from training\n                total_loss_batch = density_loss + classification_weight * class_loss\n\n                # Calculate MAE\n                mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n\n                total_loss += total_loss_batch.item()\n                total_mae += mae.item()\n\n                # Calculate classification accuracy\n                # Use argmax on logits to get predicted classes\n                predicted_classes = pred_class_logits.argmax(dim=1)\n                correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n                # Print ground truth and predicted classes for a few batches\n                if batch_idx < 5: # Inspect first 5 batches\n                    print(f\"\\nBatch {batch_idx}:\")\n                    print(f\"Ground Truth Classes: {class_labels.cpu().numpy()}\")\n                    print(f\"Predicted Classes: {predicted_classes.cpu().numpy()}\")\n\n\n        avg_loss = total_loss / len(val_loader)\n        avg_mae = total_mae / len(val_loader)\n        accuracy_classification = correct_predictions_classification / total_samples\n\n        return avg_loss, avg_mae, accuracy_classification\n\n    def train(self, train_loader, val_loader, epochs=100, save_path='best_model.pth'):\n        best_val_mae = float('inf')\n\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n        self.val_accuracies_classification = []\n        self.train_accuracies_classification = []\n\n\n        for epoch in range(epochs):\n            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n            # Training\n            train_loss, train_mae, train_accuracy_classification = self.train_epoch(train_loader)\n\n            # Validation\n            val_loss, val_mae, val_accuracy_classification = self.validate_epoch(val_loader)\n\n            # Update learning rate\n            self.scheduler.step()\n\n            # Save best model (you might want to save based on MAE as before)\n            if val_mae < best_val_mae:\n                best_val_mae = val_mae\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_mae': val_mae,\n                    'val_accuracy_classification': val_accuracy_classification,\n                }, save_path)\n                print(f\"New best model saved with MAE: {val_mae:.4f}, Classification Accuracy: {val_accuracy_classification:.4f}\")\n\n\n            # Store history\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_maes.append(train_mae)\n            self.val_maes.append(val_mae)\n            self.val_accuracies_classification.append(val_accuracy_classification)\n            self.train_accuracies_classification.append(train_accuracy_classification)\n\n\n            print(f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, Train Classification Accuracy: {train_accuracy_classification:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Val Classification Accuracy: {val_accuracy_classification:.4f}\")\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n\n\n    def test(self, test_loader, model_path='best_model.pth'):\n        \"\"\"Test the model on test dataset\"\"\"\n        # Load best model\n        checkpoint = torch.load(model_path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n        self.model.eval()\n        total_mae = 0\n        total_mse = 0\n        correct_predictions_classification = 0\n        total_samples = 0\n        all_predictions = []\n        all_ground_truths = []\n\n\n        with torch.no_grad():\n            for images, density_maps, actual_counts_from_gt in tqdm(test_loader, desc=\"Testing\"):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device)\n                actual_counts_from_gt = actual_counts_from_gt.to(self.device)\n\n                class_labels = torch.tensor([self.get_count_class(count.item()) for count in actual_counts_from_gt],\n                                            dtype=torch.long).to(self.device)\n\n\n                # Forward pass\n                pred_density, pred_class_logits = self.model(images) # Get logits\n\n                # Calculate counts\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Store predictions\n                all_predictions.extend(pred_counts.cpu().numpy())\n                all_ground_truths.extend(actual_counts_from_gt.cpu().numpy())\n\n\n                # Calculate errors\n                mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n                mse = torch.mean((pred_counts - actual_counts_from_gt) ** 2)\n\n                total_mae += mae.item()\n                total_mse += mse.item()\n\n                # Calculate classification accuracy\n                # Use argmax on logits to get predicted classes\n                predicted_classes = pred_class_logits.argmax(dim=1)\n                correct_predictions_classification += (predicted_classes == class_labels).sum().item()\n                total_samples += class_labels.size(0)\n\n\n        avg_mae = total_mae / len(test_loader)\n        avg_mse = total_mse / len(test_loader)\n        test_accuracy_classification = correct_predictions_classification / total_samples\n\n\n        print(f\"\\nTest Results:\")\n        print(f\"MAE: {avg_mae:.4f}\")\n        print(f\"MSE: {avg_mse:.4f}\")\n        print(f\"RMSE: {np.sqrt(avg_mse):.4f}\")\n        print(f\"Classification Accuracy: {test_accuracy_classification:.4f}\")\n\n\n        return avg_mae, avg_mse, test_accuracy_classification, all_predictions, all_ground_truths\n\n    def plot_training_history(self):\n        \"\"\"Plot training history\"\"\"\n        # Changed to 3 subplots as we are not plotting ratio accuracy\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\n        # Plot losses\n        ax1.plot(self.train_losses, label='Train Loss')\n        ax1.plot(self.val_losses, label='Validation Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot MAE\n        ax2.plot(self.train_maes, label='Train MAE')\n        ax2.plot(self.val_maes, label='Validation MAE')\n        ax2.set_title('Training and Validation MAE')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('MAE')\n        ax2.legend()\n        ax2.grid(True)\n\n        # Plot Classification Accuracy\n        ax3.plot(self.train_accuracies_classification, label='Train Classification Accuracy', color='red')\n        ax3.plot(self.val_accuracies_classification, label='Validation Classification Accuracy', color='green')\n        ax3.set_title('Training and Validation Classification Accuracy')\n        ax3.set_xlabel('Epoch')\n        ax3.set_ylabel('Accuracy')\n        ax3.legend()\n        ax3.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n# %%\ndef load_data_paths(train_images_dir, train_gt_dir, test_images_dir, test_gt_dir):\n    \"\"\"Load image and ground truth file paths\"\"\"\n\n    # Get training data paths\n    train_img_paths = []\n    train_gt_paths = []\n\n    for img_file in os.listdir(train_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(train_images_dir, img_file)\n            # Assuming ground truth files have same name but with .mat extension\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(train_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                train_img_paths.append(img_path)\n                train_gt_paths.append(gt_path)\n\n    # Get test data paths\n    test_img_paths = []\n    test_gt_paths = []\n\n    for img_file in os.listdir(test_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(test_images_dir, img_file)\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(test_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                test_img_paths.append(img_path)\n                test_gt_paths.append(gt_path)\n\n    return train_img_paths, train_gt_paths, test_img_paths, test_gt_paths\n# %%\ndef main():\n    \"\"\"Main function to run the complete pipeline\"\"\"\n\n    # Configuration\n    config = {\n        'train_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/images',  # UPDATE THIS PATH\n        'train_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/ground_truth',  # UPDATE THIS PATH\n        'test_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/images',  # UPDATE THIS PATH\n        'test_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/ground_truth',  # UPDATE THIS PATH\n        'batch_size': 16,\n        'epochs': 30,\n        'patch_size': 256,\n        'val_split': 0.2,\n        'num_workers': 4,\n        'save_path': 'cascaded_cnn_best.pth',\n        'num_classes': 6 # Updated number of classes to 4\n    }\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    print(\"Loading data paths...\")\n    train_img_paths, train_gt_paths, test_img_paths, test_gt_paths = load_data_paths(\n        config['train_images_dir'],\n        config['train_gt_dir'],\n        config['test_images_dir'],\n        config['test_gt_dir']\n    )\n\n    print(f\"Found {len(train_img_paths)} training images\")\n    print(f\"Found {len(test_img_paths)} test images\")\n\n    # Split training data into train and validation\n    # You should use stratified split here if possible, based on your new 4 classes\n    # To use stratified split, you'll need the class labels for the full train data first\n    train_imgs, val_imgs, train_gts, val_gts = train_test_split(\n        train_img_paths, train_gt_paths,\n        test_size=config['val_split'],\n        random_state=42\n        # Add stratify=... here after getting class labels\n    )\n\n    print(f\"Train set: {len(train_imgs)} images\")\n    print(f\"Validation set: {len(val_imgs)} images\")\n\n    # Create datasets\n    train_dataset = CrowdDataset(\n        train_imgs, train_gts,\n        augment=True,\n        patch_size=config['patch_size']\n    )\n\n    val_dataset = CrowdDataset(\n        val_imgs, val_gts,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    test_dataset = CrowdDataset(\n        test_img_paths, test_gt_paths,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True, # Consider removing shuffle if using WeightedRandomSampler\n        num_workers=config['num_workers']\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    # Initialize model and trainer\n    model = CascadedCNN(num_classes=config['num_classes']) # This will now be 6\n    model.to(device)\n    trainer = CrowdCountingTrainer(model)\n\n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n\n    # Train the model\n    print(\"Starting training...\")\n    trainer.train(\n        train_loader,\n        val_loader,\n        epochs=config['epochs'],\n        save_path=config['save_path']\n    )\n\n    # Plot training history\n    trainer.plot_training_history()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:58:33.850743Z","iopub.execute_input":"2025-06-16T05:58:33.851077Z","iopub.status.idle":"2025-06-16T05:58:34.091395Z","shell.execute_reply.started":"2025-06-16T05:58:33.851049Z","shell.execute_reply":"2025-06-16T05:58:34.090597Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T05:58:37.197060Z","iopub.execute_input":"2025-06-16T05:58:37.197332Z","iopub.status.idle":"2025-06-16T07:34:21.574968Z","shell.execute_reply.started":"2025-06-16T05:58:37.197312Z","shell.execute_reply":"2025-06-16T07:34:21.573880Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data paths...\nFound 400 training images\nFound 316 test images\nTrain set: 320 images\nValidation set: 80 images\nModel initialized with 3528709 parameters\nStarting training...\n\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:47<00:00, 20.38s/it]\nValidation:  20%|        | 1/5 [01:17<05:09, 77.32s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  80%|  | 4/5 [01:18<00:12, 12.22s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:48<00:00, 21.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nNew best model saved with MAE: 14671.4398, Classification Accuracy: 0.5375\nTrain Loss: 0.1959, Train MAE: 14121.3452, Train Classification Accuracy: 0.6000\nVal Loss: 0.1523, Val MAE: 14671.4398, Val Classification Accuracy: 0.5375\nLearning Rate: 0.000010\n\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:43<00:00, 20.18s/it]\nValidation:  20%|        | 1/5 [01:17<05:10, 77.66s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  80%|  | 4/5 [01:18<00:12, 12.19s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:48<00:00, 21.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nNew best model saved with MAE: 13344.4096, Classification Accuracy: 0.5375\nTrain Loss: 0.1380, Train MAE: 13539.1429, Train Classification Accuracy: 0.6438\nVal Loss: 0.1323, Val MAE: 13344.4096, Val Classification Accuracy: 0.5375\nLearning Rate: 0.000010\n\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:45<00:00, 20.26s/it]\nValidation:  60%|    | 3/5 [01:19<00:41, 20.73s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:51<00:00, 22.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0]\nNew best model saved with MAE: 12602.2023, Classification Accuracy: 0.6250\nTrain Loss: 0.1207, Train MAE: 13013.6735, Train Classification Accuracy: 0.6406\nVal Loss: 0.1207, Val MAE: 12602.2023, Val Classification Accuracy: 0.6250\nLearning Rate: 0.000010\n\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:50<00:00, 20.53s/it]\nValidation:  60%|    | 3/5 [01:20<00:41, 20.88s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:51<00:00, 22.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1]\nNew best model saved with MAE: 12058.3590, Classification Accuracy: 0.6500\nTrain Loss: 0.1105, Train MAE: 12506.2727, Train Classification Accuracy: 0.6844\nVal Loss: 0.1134, Val MAE: 12058.3590, Val Classification Accuracy: 0.6500\nLearning Rate: 0.000010\n\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:54<00:00, 20.75s/it]\nValidation:  20%|        | 1/5 [01:18<05:14, 78.61s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  80%|  | 4/5 [01:19<00:12, 12.20s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:49<00:00, 21.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1]\nNew best model saved with MAE: 11561.6398, Classification Accuracy: 0.6750\nTrain Loss: 0.1028, Train MAE: 11979.9502, Train Classification Accuracy: 0.7156\nVal Loss: 0.1064, Val MAE: 11561.6398, Val Classification Accuracy: 0.6750\nLearning Rate: 0.000010\n\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:43<00:00, 20.20s/it]\nValidation:  60%|    | 3/5 [01:18<00:40, 20.48s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:49<00:00, 21.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1]\nNew best model saved with MAE: 11104.9529, Classification Accuracy: 0.6875\nTrain Loss: 0.0979, Train MAE: 11463.9989, Train Classification Accuracy: 0.7312\nVal Loss: 0.1012, Val MAE: 11104.9529, Val Classification Accuracy: 0.6875\nLearning Rate: 0.000010\n\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:43<00:00, 20.19s/it]\nValidation:  20%|        | 1/5 [01:17<05:10, 77.74s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  80%|  | 4/5 [01:19<00:12, 12.41s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:48<00:00, 21.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0]\nNew best model saved with MAE: 10692.8102, Classification Accuracy: 0.7125\nTrain Loss: 0.0920, Train MAE: 10948.1691, Train Classification Accuracy: 0.7281\nVal Loss: 0.0973, Val MAE: 10692.8102, Val Classification Accuracy: 0.7125\nLearning Rate: 0.000010\n\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:44<00:00, 20.22s/it]\nValidation:  60%|    | 3/5 [01:19<00:41, 20.71s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:50<00:00, 22.17s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1]\nNew best model saved with MAE: 10293.0773, Classification Accuracy: 0.7250\nTrain Loss: 0.0862, Train MAE: 10486.2098, Train Classification Accuracy: 0.7844\nVal Loss: 0.0924, Val MAE: 10293.0773, Val Classification Accuracy: 0.7250\nLearning Rate: 0.000010\n\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:49<00:00, 20.50s/it]\nValidation:  20%|        | 1/5 [01:19<05:19, 79.77s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  80%|  | 4/5 [01:20<00:12, 12.44s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:51<00:00, 22.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1]\nNew best model saved with MAE: 9782.8191, Classification Accuracy: 0.7000\nTrain Loss: 0.0839, Train MAE: 9926.5180, Train Classification Accuracy: 0.7344\nVal Loss: 0.0896, Val MAE: 9782.8191, Val Classification Accuracy: 0.7000\nLearning Rate: 0.000010\n\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:45<00:00, 20.27s/it]\nValidation:  60%|    | 3/5 [01:20<00:41, 20.80s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:51<00:00, 22.21s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1]\nNew best model saved with MAE: 9227.6559, Classification Accuracy: 0.7375\nTrain Loss: 0.0794, Train MAE: 9475.0475, Train Classification Accuracy: 0.7906\nVal Loss: 0.0850, Val MAE: 9227.6559, Val Classification Accuracy: 0.7375\nLearning Rate: 0.000010\n\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:52<00:00, 20.63s/it]\nValidation:  60%|    | 3/5 [01:19<00:41, 20.71s/it]","output_type":"stream"},{"name":"stdout","text":"\nBatch 0:\nGround Truth Classes: [0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0]\nPredicted Classes: [0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0]\n\nBatch 1:\nGround Truth Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1]\nPredicted Classes: [1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0]\n\nBatch 2:\nGround Truth Classes: [0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0]\nPredicted Classes: [0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0]\n\nBatch 3:\nGround Truth Classes: [1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\nPredicted Classes: [1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:50<00:00, 22.12s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 4:\nGround Truth Classes: [0 1 2 1 0 0 1 2 1 1 0 0 0 1 0 1]\nPredicted Classes: [0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1]\nNew best model saved with MAE: 8688.7215, Classification Accuracy: 0.7625\nTrain Loss: 0.0744, Train MAE: 8936.5613, Train Classification Accuracy: 0.7906\nVal Loss: 0.0811, Val MAE: 8688.7215, Val Classification Accuracy: 0.7625\nLearning Rate: 0.000010\n\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/20 [00:48<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3832242952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/340079902.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m     trainer.train(\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/340079902.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs, save_path)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_classification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/340079902.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdensity_maps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mdensity_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensity_maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"from IPython import get_ipython\nfrom IPython.display import display\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport scipy.io\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import gaussian_filter\nimport random\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# %%\nclass CrowdDataset(Dataset):\n    def __init__(self, image_paths, ground_truth_paths, transform=None, augment=False, patch_size=256):\n        self.image_paths = image_paths\n        self.ground_truth_paths = ground_truth_paths\n        self.transform = transform\n        self.augment = augment\n        self.patch_size = patch_size\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Initialize points and actual_count\n        points = []\n        actual_count = 0.0\n\n        # Load ground truth from .mat file\n        gt_path = self.ground_truth_paths[idx]\n        try:\n            mat_data = scipy.io.loadmat(gt_path)\n            \n            # Extract point coordinates from image_info[0,0][0,0][0] for density map generation\n            extracted_points = mat_data['image_info'][0, 0][0, 0][0]\n            if extracted_points.size > 0 and len(extracted_points.shape) == 2 and extracted_points.shape[1] == 2:\n                points = extracted_points.tolist()\n            else:\n                points = []\n\n            # Extract actual count directly from image_info[0,0][0,0][1]\n            # Ensure the count data is numeric and can be converted to a float\n            raw_count = mat_data['image_info'][0, 0][0, 0][1]\n            actual_count = float(raw_count)\n\n        except Exception as e:\n            print(f\"Error loading ground truth file {gt_path}: {e}. Defaulting to 0 points and 0 count.\")\n            points = []\n            actual_count = 0.0\n\n        # Generate density map from points\n        # The density map is still generated even if actual_count comes from elsewhere,\n        # as it's used for the regression branch.\n        density_map = self.generate_density_map(img.shape[:2], points)\n\n        # Apply augmentation if specified\n        if self.augment:\n            img, density_map = self.apply_augmentation(img, density_map)\n\n        # Resize to patch size\n        img = cv2.resize(img, (self.patch_size, self.patch_size))\n        \n        # --- IMPORTANT FIX: Re-normalize density map after resizing ---\n        # Store original sum for scaling\n        original_density_sum = np.sum(density_map)\n        \n        # Resize density map\n        density_map = cv2.resize(density_map, (self.patch_size, self.patch_size), interpolation=cv2.INTER_LINEAR)\n        \n        # If the original density map had points, re-scale the resized map to match the original sum\n        if original_density_sum > 0:\n            current_density_sum = np.sum(density_map)\n            if current_density_sum > 0: # Avoid division by zero if current_density_sum is also 0 somehow\n                density_map = density_map * (original_density_sum / current_density_sum)\n        # --- END IMPORTANT FIX ---\n\n        # Convert to tensor\n        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n        # Unsqueeze(0) for channel dimension (1, H, W)\n        density_map = torch.from_numpy(density_map).float().unsqueeze(0)\n\n        # Apply transforms if any\n        if self.transform:\n            img = self.transform(img)\n\n        # Return the image, density map, and the actual count for metrics\n        return img, density_map, torch.tensor(actual_count, dtype=torch.float32)\n\n    def generate_density_map(self, img_shape, points):\n        \"\"\"Generate density map from point annotations using adaptive Gaussian kernels\"\"\"\n        h, w = img_shape\n        density_map = np.zeros((h, w), dtype=np.float32)\n\n        if len(points) == 0:\n            return density_map\n\n        # Convert points to numpy array\n        points = np.array(points)\n\n        # For each point, calculate adaptive sigma based on k-nearest neighbors\n        for i, point in enumerate(points):\n            x, y = int(point[0]), int(point[1])\n\n            # Skip if point is outside image bounds\n            if x < 0 or x >= w or y < 0 or y >= h:\n                continue\n\n            # Calculate distance to k nearest neighbors (k=3)\n            if len(points) > 1:\n                distances = np.sqrt(np.sum((points - point) ** 2, axis=1))\n                distances = np.sort(distances)[1:]  # Exclude self (distance=0)\n\n                # Use average of 3 nearest neighbors or all if less than 3\n                k = min(3, len(distances))\n                avg_distance = np.mean(distances[:k])\n\n                # Adaptive sigma based on local density\n                sigma = max(1.0, avg_distance / 3.0)\n            else:\n                sigma = 4.0  # Default sigma for single point\n\n            # Create Gaussian kernel\n            kernel_size = int(6 * sigma)\n            if kernel_size % 2 == 0:\n                kernel_size += 1\n\n            # Generate 2D Gaussian\n            gaussian = self.gaussian_2d(kernel_size, sigma)\n\n            # Add to density map\n            x_start = max(0, x - kernel_size // 2)\n            x_end = min(w, x + kernel_size // 2 + 1)\n            y_start = max(0, y - kernel_size // 2)\n            y_end = min(h, y + kernel_size // 2 + 1)\n\n            # Adjust Gaussian bounds\n            g_x_start = max(0, kernel_size // 2 - x)\n            g_x_end = g_x_start + (x_end - x_start)\n            g_y_start = max(0, kernel_size // 2 - y)\n            g_y_end = g_y_start + (y_end - y_start)\n\n            density_map[y_start:y_end, x_start:x_end] += gaussian[g_y_start:g_y_end, g_x_start:g_x_end]\n        \n        # Ensure the sum of the density map corresponds to the number of points initially,\n        # but the actual_count from .mat is the primary source for metrics.\n        # This is important if `actual_count` doesn't strictly equal len(points)\n        # or if `cv2.resize` changes the sum.\n\n        return density_map\n\n    def gaussian_2d(self, kernel_size, sigma):\n        \"\"\"Generate 2D Gaussian kernel\"\"\"\n        kernel = np.zeros((kernel_size, kernel_size))\n        center = kernel_size // 2\n\n        for i in range(kernel_size):\n            for j in range(kernel_size):\n                x, y = i - center, j - center\n                kernel[i, j] = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n\n        return kernel / np.sum(kernel) # Normalize to sum to 1\n\n    def apply_augmentation(self, img, density_map):\n        \"\"\"Apply data augmentation\"\"\"\n        # Random horizontal flip\n        if random.random() > 0.5:\n            img = cv2.flip(img, 1)\n            density_map = cv2.flip(density_map, 1)\n\n        # Random brightness adjustment\n        if random.random() > 0.5:\n            brightness_factor = random.uniform(0.8, 1.2)\n            img = np.clip(img * brightness_factor, 0, 255).astype(np.uint8)\n\n        # Random noise\n        if random.random() > 0.7:\n            noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n            img = np.clip(img + noise, 0, 255).astype(np.uint8)\n\n        return img, density_map\n\n# %%\nclass CascadedCNN(nn.Module):\n    \"\"\"Simplified Cascaded CNN for Crowd Density Estimation (Regression Only)\"\"\"\n    def __init__(self):\n        super(CascadedCNN, self).__init__()\n\n        # Shared front-end network\n        self.shared_conv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=5, padding=2),\n            nn.BatchNorm2d(32),\n            nn.Tanh(),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Tanh(),\n            nn.MaxPool2d(2, stride=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(),\n            nn.MaxPool2d(2, stride=2),\n        )\n\n        # Density estimation branch\n        self.density_conv = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.Tanh(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Tanh(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.Tanh(),\n        )\n\n        # Fusion layers (now directly processes density features, adjusted input channels)\n        # This block can be seen as further processing the density features before upsampling\n        self.fusion_conv = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1), # Input channels from density_conv output (32)\n            nn.BatchNorm2d(64),\n            nn.Tanh(),\n        )\n\n        # Upsampling layers\n        self.upsample = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # Upsample to 128x128\n            nn.BatchNorm2d(32),\n            nn.Tanh(),\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1), # Upsample to 256x256\n            nn.BatchNorm2d(16),\n            nn.Tanh(),\n            nn.Conv2d(16, 1, kernel_size=1), # Final 1x1 convolution to get 1 channel density map\n        )\n\n    def forward(self, x):\n        # Shared front-end\n        shared_features = self.shared_conv(x)\n\n        # Density estimation branch\n        density_features = self.density_conv(shared_features)\n\n        # Fusion (now directly processes density_features)\n        fused_features = self.fusion_conv(density_features)\n\n        # Upsampling\n        density_map = self.upsample(fused_features)\n\n        return density_map # Only return density map for regression\n\n# %%\nclass CrowdCountingTrainer:\n    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = model.to(device)\n        self.device = device\n        self.density_criterion = nn.MSELoss() # Only MSE Loss for regression\n        self.optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.5) # Ensure step_size < epochs\n\n        # Training history (removed classification related lists)\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n\n    # Removed get_count_class method as classification is no longer part of the pipeline\n\n    def train_epoch(self, train_loader):\n        self.model.train()\n        total_loss = 0\n        total_mae = 0\n        \n        # DataLoader now yields image, density_map, and actual_count\n        for batch_idx, (images, density_maps, actual_counts_from_gt) in enumerate(tqdm(train_loader, desc=\"Training\")):\n            images = images.to(self.device)\n            density_maps = density_maps.to(self.device)\n            actual_counts_from_gt = actual_counts_from_gt.to(self.device)\n\n            self.optimizer.zero_grad()\n\n            # Forward pass: model now only returns pred_density\n            pred_density = self.model(images)\n            pred_counts = torch.sum(pred_density, dim=(1, 2, 3)) # Sum of predicted density map for count\n\n            # Calculate loss (only density loss)\n            density_loss = self.density_criterion(pred_density, density_maps)\n            total_loss_batch = density_loss # Total loss is just density loss\n\n            # Backward pass\n            total_loss_batch.backward()\n            self.optimizer.step()\n\n            # Calculate MAE using predicted counts vs. actual counts from GT file\n            mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n\n            total_loss += total_loss_batch.item()\n            total_mae += mae.item()\n\n            # Print actual vs predicted counts for first few batches\n            if batch_idx < 3: # Print for first 3 batches\n                # Fix: .detach() before .cpu().numpy()\n                print(f\"  Batch {batch_idx+1} (Train): Actual Counts (GT from .mat): {actual_counts_from_gt.detach().cpu().numpy().round(2)}\")\n                print(f\"  Batch {batch_idx+1} (Train): Predicted Counts (from density map): {pred_counts.detach().cpu().numpy().round(2)}\")\n            \n        avg_loss = total_loss / len(train_loader)\n        avg_mae = total_mae / len(train_loader)\n\n        return avg_loss, avg_mae\n\n    def validate_epoch(self, val_loader):\n        self.model.eval()\n        total_loss = 0\n        total_mae = 0\n\n        with torch.no_grad():\n            # DataLoader now yields image, density_map, and actual_count\n            for batch_idx, (images, density_maps, actual_counts_from_gt) in enumerate(tqdm(val_loader, desc=\"Validation\")):\n                images = images.to(self.device)\n                density_maps = density_maps.to(self.device) # Not strictly needed for validation metrics, but consistent.\n                actual_counts_from_gt = actual_counts_from_gt.to(self.device)\n\n                # Forward pass\n                pred_density = self.model(images)\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Calculate loss\n                density_loss = self.density_criterion(pred_density, density_maps)\n                total_loss_batch = density_loss\n\n                # Calculate MAE\n                mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n\n                total_loss += total_loss_batch.item()\n                total_mae += mae.item()\n\n                # Print actual vs predicted counts for first few batches\n                if batch_idx < 3: # Print for first 3 batches\n                    # Fix: .detach() before .cpu().numpy()\n                    print(f\"  Batch {batch_idx+1} (Val): Actual Counts (GT from .mat): {actual_counts_from_gt.detach().cpu().numpy().round(2)}\")\n                    print(f\"  Batch {batch_idx+1} (Val): Predicted Counts (from density map): {pred_counts.detach().cpu().numpy().round(2)}\")\n                \n        avg_loss = total_loss / len(val_loader)\n        avg_mae = total_mae / len(val_loader)\n\n        return avg_loss, avg_mae\n\n    def train(self, train_loader, val_loader, epochs=100, save_path='best_model.pth'):\n        best_val_mae = float('inf')\n\n        self.train_losses = []\n        self.val_losses = []\n        self.train_maes = []\n        self.val_maes = []\n\n        for epoch in range(epochs):\n            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n\n            # Training\n            train_loss, train_mae = self.train_epoch(train_loader)\n\n            # Validation\n            val_loss, val_mae = self.validate_epoch(val_loader)\n\n            # Update learning rate\n            self.scheduler.step()\n\n            # Save best model based on MAE\n            if val_mae < best_val_mae:\n                best_val_mae = val_mae\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_mae': val_mae,\n                }, save_path)\n                print(f\"New best model saved with MAE: {val_mae:.4f}\")\n\n            # Store history\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_maes.append(train_mae)\n            self.val_maes.append(val_mae)\n\n            print(f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n\n    def test(self, test_loader, model_path='best_model.pth'):\n        \"\"\"Test the model on test dataset\"\"\"\n        # Load best model\n        # Using map_location='cpu' ensures it works even if GPU is not available for testing\n        checkpoint = torch.load(model_path, map_location=self.device) \n        self.model.load_state_dict(checkpoint['model_state_dict'])\n\n        self.model.eval()\n        total_mae = 0\n        total_mse = 0\n        all_predictions = []\n        all_ground_truths = []\n\n        with torch.no_grad():\n            for images, density_maps, actual_counts_from_gt in tqdm(test_loader, desc=\"Testing\"):\n                images = images.to(self.device)\n                # density_maps = density_maps.to(self.device) # Not strictly needed if only using actual_counts_from_gt for GT\n                actual_counts_from_gt = actual_counts_from_gt.to(self.device)\n\n                # Forward pass\n                pred_density = self.model(images)\n\n                # Calculate counts\n                pred_counts = torch.sum(pred_density, dim=(1, 2, 3))\n\n                # Store predictions\n                all_predictions.extend(pred_counts.detach().cpu().numpy()) # Fix: .detach() added\n                all_ground_truths.extend(actual_counts_from_gt.detach().cpu().numpy()) # Fix: .detach() added\n\n                # Calculate errors using actual_counts_from_gt\n                mae = torch.mean(torch.abs(pred_counts - actual_counts_from_gt))\n                mse = torch.mean((pred_counts - actual_counts_from_gt) ** 2)\n\n                total_mae += mae.item()\n                total_mse += mse.item()\n\n        avg_mae = total_mae / len(test_loader)\n        avg_mse = total_mse / len(test_loader)\n        \n        print(f\"\\nTest Results:\")\n        print(f\"MAE: {avg_mae:.4f}\")\n        print(f\"MSE: {avg_mse:.4f}\")\n        print(f\"RMSE: {np.sqrt(avg_mse):.4f}\")\n\n        return avg_mae, avg_mse, all_predictions, all_ground_truths\n\n    def plot_training_history(self):\n        \"\"\"Plot training history (Loss and MAE only)\"\"\"\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5)) # Changed to 2 subplots\n\n        # Plot losses\n        ax1.plot(self.train_losses, label='Train Loss')\n        ax1.plot(self.val_losses, label='Validation Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True)\n\n        # Plot MAE\n        ax2.plot(self.train_maes, label='Train MAE')\n        ax2.plot(self.val_maes, label='Validation MAE')\n        ax2.set_title('Training and Validation MAE')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('MAE')\n        ax2.legend()\n        ax2.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n\n# %%\ndef load_data_paths(train_images_dir, train_gt_dir, test_images_dir, test_gt_dir):\n    \"\"\"Load image and ground truth file paths\"\"\"\n\n    # Get training data paths\n    train_img_paths = []\n    train_gt_paths = []\n\n    if not os.path.exists(train_images_dir):\n        print(f\"Warning: Training images directory not found: {train_images_dir}\")\n    if not os.path.exists(train_gt_dir):\n        print(f\"Warning: Training GT directory not found: {train_gt_dir}\")\n\n    for img_file in os.listdir(train_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(train_images_dir, img_file)\n            # Assuming ground truth files have same name but with .mat extension\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(train_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                train_img_paths.append(img_path)\n                train_gt_paths.append(gt_path)\n            else:\n                print(f\"Warning: Corresponding GT file not found for {img_file} at {gt_path}\")\n\n\n    # Get test data paths\n    test_img_paths = []\n    test_gt_paths = []\n\n    if not os.path.exists(test_images_dir):\n        print(f\"Warning: Test images directory not found: {test_images_dir}\")\n    if not os.path.exists(test_gt_dir):\n        print(f\"Warning: Test GT directory not found: {test_gt_dir}\")\n\n    for img_file in os.listdir(test_images_dir):\n        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            img_path = os.path.join(test_images_dir, img_file)\n            gt_file = 'GT_' + os.path.splitext(img_file)[0] + '.mat'\n            gt_path = os.path.join(test_gt_dir, gt_file)\n\n            if os.path.exists(gt_path):\n                test_img_paths.append(img_path)\n                test_gt_paths.append(gt_path)\n            else:\n                print(f\"Warning: Corresponding GT file not found for {img_file} at {gt_path}\")\n\n    return train_img_paths, train_gt_paths, test_img_paths, test_gt_paths\n\n# %%\ndef main():\n    \"\"\"Main function to run the complete pipeline for density regression\"\"\"\n\n    # Configuration\n    config = {\n        'train_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/images',  # UPDATE THIS PATH\n        'train_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/train_data/ground_truth',  # UPDATE THIS PATH\n        'test_images_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/images',  # UPDATE THIS PATH\n        'test_gt_dir': '/kaggle/input/crowd-wala/crowd_wala_dataset/test_data/ground_truth',  # UPDATE THIS PATH\n        'batch_size': 16,\n        'epochs': 10, # Adjusted epochs. Ensure step_size in scheduler is set appropriately, e.g., 10 or 15.\n        'patch_size': 256,\n        'val_split': 0.2,\n        'num_workers': 4,\n        'save_path': 'cascaded_cnn_regression_only.pth', # Updated save path name\n    }\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    print(\"Loading data paths...\")\n    train_img_paths, train_gt_paths, test_img_paths, test_gt_paths = load_data_paths(\n        config['train_images_dir'],\n        config['train_gt_dir'],\n        config['test_images_dir'],\n        config['test_gt_dir']\n    )\n\n    print(f\"Found {len(train_img_paths)} training images with GT\")\n    print(f\"Found {len(test_img_paths)} test images with GT\")\n\n    if not train_img_paths:\n        print(\"Error: No training images with corresponding ground truth found. Please check your data paths and file naming conventions.\")\n        return\n\n    # Split training data into train and validation\n    # No stratification needed as classification is removed\n    train_imgs, val_imgs, train_gts, val_gts = train_test_split(\n        train_img_paths, train_gt_paths,\n        test_size=config['val_split'],\n        random_state=42\n    )\n\n    print(f\"Train set: {len(train_imgs)} images\")\n    print(f\"Validation set: {len(val_imgs)} images\")\n\n    # Create datasets\n    train_dataset = CrowdDataset(\n        train_imgs, train_gts,\n        augment=True,\n        patch_size=config['patch_size']\n    )\n\n    val_dataset = CrowdDataset(\n        val_imgs, val_gts,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    test_dataset = CrowdDataset(\n        test_img_paths, test_gt_paths,\n        augment=False,\n        patch_size=config['patch_size']\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers']\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers']\n    )\n\n    # Initialize model and trainer\n    model = CascadedCNN() # num_classes is no longer passed\n    model.to(device)\n    trainer = CrowdCountingTrainer(model) # No num_classes for trainer\n\n    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n\n    # Train the model\n    print(\"Starting training...\")\n    trainer.train(\n        train_loader,\n        val_loader,\n        epochs=config['epochs'],\n        save_path=config['save_path']\n    )\n\n    # Plot training history\n    trainer.plot_training_history()\n\n    # Test the model\n    if test_img_paths:\n        print(\"\\nStarting evaluation on test set...\")\n        test_mae, test_mse, all_predictions, all_ground_truths = trainer.test(\n            test_loader,\n            model_path=config['save_path']\n        )\n        print(f\"Final Test MAE: {test_mae:.4f}\")\n        print(f\"Final Test MSE: {test_mse:.4f}\")\n        print(f\"Final Test RMSE: {np.sqrt(test_mse):.4f}\")\n    else:\n        print(\"No test data found for evaluation.\")\n\n# %%\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T12:55:06.520809Z","iopub.execute_input":"2025-06-17T12:55:06.521485Z","iopub.status.idle":"2025-06-17T14:22:40.065410Z","shell.execute_reply.started":"2025-06-17T12:55:06.521448Z","shell.execute_reply":"2025-06-17T14:22:40.064518Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading data paths...\nFound 400 training images with GT\nFound 316 test images with GT\nTrain set: 320 images\nValidation set: 80 images\nModel initialized with 395,265 parameters\nStarting training...\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:23<10:20, 34.48s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [ 49. 134.  30. 120. 211. 190. 138.  36. 269. 108.  83.  62.  30. 191.\n  43. 118.]\n  Batch 1 (Train): Predicted Counts (from density map): [ -6588.51  -7277.73  -8523.99  -7032.04  -8261.1  -10704.01  -7689.74\n  -8354.96  -8801.59  -7251.56  -7196.02  -6548.83  -4621.99 -10075.26\n  -6342.54  -3490.27]\n  Batch 2 (Train): Actual Counts (GT from .mat): [302. 201. 350. 359.  31.  59. 287. 147.  19. 161.  95. 162. 274.  47.\n  98.  29.]\n  Batch 2 (Train): Predicted Counts (from density map): [-6528.55 -6989.75 -6943.4  -7356.08 -8045.37 -7275.53 -6829.49 -7782.59\n -6832.26 -6962.98 -7089.05 -7894.1  -7298.76 -7424.98 -6949.25 -7059.93]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:23<05:19, 18.79s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [ 86. 106.  95. 293. 234. 123. 234. 155. 134. 235.  87.  87.  27. 174.\n 152. 178.]\n  Batch 3 (Train): Predicted Counts (from density map): [-6943.46 -6372.36 -9615.3  -7032.95 -6575.4  -8234.43 -7281.01 -6792.56\n -6255.21 -5835.07 -7326.57 -6204.83 -6540.87 -5527.59 -7462.42 -7461.31]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:27<00:00, 19.40s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.07s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [-4700.15 -4190.3  -4466.2  -4467.69 -4262.24 -4580.71 -4824.57 -4208.84\n -4898.32 -4202.72 -4477.39 -4700.01 -4432.83 -4089.29 -4140.59 -4964.89]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [-4155.04 -4112.47 -4719.41 -4772.93 -4658.89 -4552.65 -4823.12 -4721.15\n -4279.76 -4460.28 -4403.   -4573.14 -4097.71 -4186.03 -4815.96 -4247.09]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [-4891.78 -5153.78 -4306.03 -4416.75 -4648.18 -4153.28 -4104.98 -4615.25\n -4521.13 -4841.3  -4249.49 -4379.12 -4355.56 -4270.2  -4425.15 -4707.24]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.85s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 4640.4696\nTrain Loss: 0.0364, Train MAE: 5920.6417\nVal Loss: 0.0068, Val MAE: 4640.4696\nLearning Rate: 0.001000\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:29<11:04, 36.92s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [100.  95. 111.  26.  25.  90.  28.  54. 107.  35. 147. 165. 234.  43.\n  34.  60.]\n  Batch 1 (Train): Predicted Counts (from density map): [-3981.96 -3964.45 -4248.65 -4146.87 -4044.22 -4084.32 -4053.5  -4112.15\n -4043.03 -3999.89 -4238.09 -3957.61 -3947.53 -4129.67 -4024.79 -4180.49]\n  Batch 2 (Train): Actual Counts (GT from .mat): [220. 118. 134. 136.  50. 156. 158. 121.  42.  65.  84.  35.  86.  87.\n 165. 344.]\n  Batch 2 (Train): Predicted Counts (from density map): [-3811.53 -3690.66 -3757.06 -3786.11 -3920.76 -3954.28 -3892.17 -3896.64\n -3894.02 -3909.53 -3903.83 -3801.72 -3878.25 -3846.48 -3846.75 -4095.91]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:29<05:41, 20.12s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [ 28.  62.  20.  26.  45.  79. 115.  95.  50. 146. 169.  33. 145. 153.\n  43. 174.]\n  Batch 3 (Train): Predicted Counts (from density map): [-3837.07 -3817.02 -4005.64 -3979.92 -4256.19 -3700.92 -3591.46 -3700.2\n -3821.7  -3866.19 -4042.65 -3770.67 -3793.97 -3462.86 -3734.76 -3936.37]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:20<00:00, 19.02s/it]\nValidation:  20%|        | 1/5 [01:14<04:58, 74.73s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [-2026.84 -1716.16 -1839.29 -1797.36 -1971.7  -1751.88 -1858.95 -1757.78\n -1762.51 -1755.92 -1792.99 -1776.6  -2118.25 -1705.27 -1747.56 -1979.12]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  40%|      | 2/5 [01:16<01:35, 31.78s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [-1636.56 -1768.8  -2046.21 -1777.91 -1805.7  -1855.99 -1862.71 -1752.15\n -1905.32 -1748.48 -2009.86 -1868.56 -1873.79 -1717.94 -2011.83 -1913.08]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [-1905.38 -1806.4  -1884.49 -1913.61 -1933.33 -1648.63 -1829.54 -1880.08\n -1987.25 -2018.71 -1815.43 -1829.11 -1823.12 -1810.17 -1744.14 -1925.69]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 1981.3380\nTrain Loss: 0.0033, Train MAE: 2825.9820\nVal Loss: 0.0018, Val MAE: 1981.3380\nLearning Rate: 0.001000\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:07<08:20, 27.83s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [ 38. 269.  58.  29.  43.  36.  47.  27.  47. 302. 142. 109. 117.  30.\n  85. 168.]\n  Batch 1 (Train): Predicted Counts (from density map): [-1348.61 -1458.82 -1422.3  -1528.32 -1384.76 -1501.94 -1452.4  -1484.57\n -1481.28 -1206.25 -1460.39 -1302.61 -1351.57 -1381.74 -1512.75 -1613.53]\n  Batch 2 (Train): Actual Counts (GT from .mat): [ 43.  83. 188. 105. 118. 301. 266. 103. 283. 229.  80. 175. 101.  61.\n  31.  43.]\n  Batch 2 (Train): Predicted Counts (from density map): [-1349.28 -1145.44 -1060.2  -1214.17 -1408.8  -1294.   -1457.09  -872.63\n -1267.72 -1204.14 -1421.44 -1265.39 -1258.32 -1306.01 -1317.37 -1237.16]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:07<04:17, 15.18s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [344. 234.  34.  62. 253. 205. 136.  34. 173.  69. 111. 174. 355. 145.\n  80.  65.]\n  Batch 3 (Train): Predicted Counts (from density map): [-1040.53 -1205.83 -1287.57 -1307.21 -1286.25  -974.56 -1283.97  -924.71\n -1044.28 -1104.16 -1200.81 -1248.24  -959.16 -1176.82 -1260.08 -1181.16]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:27<00:00, 19.37s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.20s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [-236.01  -34.36 -105.38 -249.23 -240.93  -68.26 -159.49  -53.51  -99.67\n  -54.71 -219.86 -170.07 -366.43  -43.69  -49.45 -263.58]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [  59.5   -98.28 -282.54 -136.23 -145.68 -156.27 -104.99  -94.25 -155.52\n -128.04 -241.38 -262.76 -203.79  -14.13 -465.84 -208.47]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [-247.96 -236.55 -131.39 -246.   -203.86   23.79 -116.79 -250.42 -262.12\n -234.32 -156.88 -184.69 -111.18 -109.02 -219.92 -236.73]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 305.1462\nTrain Loss: 0.0009, Train MAE: 800.0024\nVal Loss: 0.0007, Val MAE: 305.1462\nLearning Rate: 0.001000\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:19<09:46, 32.57s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [198.  53. 109.  60. 132.  36. 134. 107. 190.  40. 178.  49. 157. 113.\n 115. 281.]\n  Batch 1 (Train): Predicted Counts (from density map): [ -44.39 -201.95 -255.31  -19.73   81.6  -211.24 -146.59   10.04   26.38\n -104.73  -83.49 -154.14  -84.19   47.38 -223.69  -98.48]\n  Batch 2 (Train): Actual Counts (GT from .mat): [153. 102.  25.  86. 116. 239. 162.  47. 103. 269. 121.  70. 107.  36.\n  68.  69.]\n  Batch 2 (Train): Predicted Counts (from density map): [-153.84  -75.61   64.01  -80.95  -62.86  -60.     90.32 -289.7   206.2\n -188.83   24.98 -251.99   60.39  -70.56  -90.01  -56.78]\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|        | 4/20 [01:19<02:52, 10.78s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [ 26.  23. 156. 205. 166.  50. 274. 415. 156. 185.  31.  78.  59.  84.\n 292. 220.]\n  Batch 3 (Train): Predicted Counts (from density map): [ -70.29 -109.51 -101.7   -57.73 -108.29  -88.17  -73.88 -108.1   -36.2\n -120.31 -201.55   -5.67 -100.9  -168.16 -182.92 -218.59]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:19<00:00, 18.98s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.09s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [106.12 141.05 176.93  -6.05  26.3  226.14 161.16 155.98 158.71 136.94\n   6.69 101.93 -29.95  65.76  78.47 103.34]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [ 202.56   43.25   72.93  145.34  122.42  116.12  214.95  145.94  107.34\n  102.46  115.26   15.77   35.2   170.03 -121.96   95.56]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 79.52  52.4   88.62 -26.49  91.43 157.45  46.09  66.96  44.32  97.23\n  53.21  69.95  96.48 143.15  18.54  81.65]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 95.0528\nTrain Loss: 0.0005, Train MAE: 139.9506\nVal Loss: 0.0005, Val MAE: 95.0528\nLearning Rate: 0.001000\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:21<10:01, 33.40s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [143.  53. 185.  50.  65. 253. 122. 100. 264. 134. 137. 300.  19.  28.\n 152.  68.]\n  Batch 1 (Train): Predicted Counts (from density map): [109.48  36.97   0.4   -8.17 132.05  43.48  68.28  69.26  80.28  72.9\n  45.65  43.21   0.29  70.94  82.22  73.68]\n  Batch 2 (Train): Actual Counts (GT from .mat): [157.  80.  54. 166.  46.  55.  16.  14. 415. 415. 115.  65.  33.  21.\n  23.  35.]\n  Batch 2 (Train): Predicted Counts (from density map): [ 1.3429e+02  9.2890e+01  2.9290e+01  7.9400e+01  1.6874e+02  1.2000e-01\n  7.2040e+01  1.4764e+02  8.5930e+01  1.5397e+02  1.1000e-01  8.2330e+01\n  1.2201e+02 -7.8500e+00  2.7141e+02 -3.4100e+00]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:21<05:09, 18.20s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [ 87. 350. 292.  25. 347.  38. 130. 156.  67. 188. 107.  36. 139. 168.\n  80.  35.]\n  Batch 3 (Train): Predicted Counts (from density map): [113.79 162.83 124.55 -88.71 -38.8  224.87 201.07 122.27 132.54 196.36\n  81.77  38.67 130.86  55.05 153.58 115.21]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:22<00:00, 19.14s/it]\nValidation:  80%|  | 4/5 [01:15<00:14, 14.29s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [ 91.    98.7  149.88 -43.14  11.8  186.34 124.19 122.14 120.42  95.36\n -31.19  64.68 -43.55  31.39  35.52  79.75]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [ 143.87   10.91   52.55  108.83   87.08   78.85  186.87  116.78   80.33\n   71.45  102.94   -9.52    4.35  125.95 -117.43   71.27]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 49.17  34.03  56.81 -51.75  74.82 112.22  11.87  40.41  28.21  84.17\n  10.69  34.96  55.25 113.28 -11.49  54.05]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:45<00:00, 21.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0004, Train MAE: 99.6776\nVal Loss: 0.0004, Val MAE: 103.3017\nLearning Rate: 0.001000\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|         | 1/20 [01:11<22:44, 71.79s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [178.  26. 167. 106.  53. 276. 310. 125. 283.  47.  50. 185.  59. 234.\n 115. 257.]\n  Batch 1 (Train): Predicted Counts (from density map): [-73.19 102.55 188.24 139.74 -22.55 -12.27 166.07  67.8   56.09  77.99\n  78.75  73.92 161.5   55.6  -49.38 -31.39]\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:15<09:33, 31.84s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 2 (Train): Actual Counts (GT from .mat): [166. 174.  98.  85. 109.  78.  68. 103.  61. 136. 578.  19. 344. 211.\n 283.  65.]\n  Batch 2 (Train): Predicted Counts (from density map): [ 39.84   8.14  39.74  54.53  14.35  97.61 116.43 120.75  64.1   74.54\n 120.86 -19.16 194.78  19.04  65.99  72.87]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:17<05:11, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [ 25.  29. 522.  94. 121. 152.  96.  64.  30.  29. 161.  26.  99. 271.\n 175. 138.]\n  Batch 3 (Train): Predicted Counts (from density map): [-75.69 147.21 -94.73  43.42 143.56 139.32 119.66  53.88  46.12  88.44\n 172.24  14.24  53.7  112.03 159.19 128.82]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:19<00:00, 18.96s/it]\nValidation:  20%|        | 1/5 [01:14<04:59, 74.83s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [ 79.19  70.41 130.5  -70.46   2.4  125.54  96.29 102.36  88.86  65.74\n -59.53  33.38 -42.24  -9.83 -10.79  45.26]\n","output_type":"stream"},{"name":"stderr","text":"Validation:  40%|      | 2/5 [01:15<01:32, 30.99s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [ 102.78  -15.38   19.24   91.12   67.09   47.03  147.58   79.52   59.96\n   27.53   99.43  -37.2   -15.38   87.63 -111.97   55.3 ]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 15.8    4.76  31.18 -81.35  62.73  76.83 -12.07  27.74   6.79  36.05\n -20.83   4.45  20.79  83.82 -42.09  39.93]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0004, Train MAE: 105.8037\nVal Loss: 0.0003, Val MAE: 114.0504\nLearning Rate: 0.001000\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:30<11:11, 37.32s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [ 55. 234.  47. 120.  29. 107. 134. 341. 143.  65.  43.  75.  25. 121.\n 222. 167.]\n  Batch 1 (Train): Predicted Counts (from density map): [  67.42  -37.49  -80.25 -124.26   49.33   87.64  -56.62  103.75   54.83\n   32.75   -5.45   77.74  -99.7    89.2   -56.22  116.16]\n  Batch 2 (Train): Actual Counts (GT from .mat): [125.  79.  28.  95.  23. 522.  89. 198.  62.  88.  53.  97.  50. 137.\n  95. 107.]\n  Batch 2 (Train): Predicted Counts (from density map): [  88.77  -47.03  -10.28   33.29  -81.77 -120.19   43.9    82.12  101.24\n   71.59    7.3   -23.25  125.68   -6.82  113.26   64.8 ]\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|        | 4/20 [01:30<03:17, 12.35s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [220. 300. 264.  89. 156. 269.  70.  90.  40. 204. 167. 145.  41. 130.\n 175. 130.]\n  Batch 3 (Train): Predicted Counts (from density map): [-12.97 -27.28 -22.97 -32.81  71.72 -48.63  29.01  -4.61 -57.87 -69.36\n -35.51  79.92 -65.5   29.7   40.04  74.28]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:20<00:00, 19.03s/it]\nValidation:  80%|  | 4/5 [01:16<00:14, 14.56s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [122.64 109.75 166.71 -23.75  49.67 144.62 124.41 139.39 123.27 104.68\n -15.66  68.43   5.15  36.54  27.77  79.93]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [131.67  33.77  47.75 126.63 102.48  84.09 174.72 118.77  96.76  63.24\n 143.43   6.57  28.52 121.23 -49.25  97.08]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 51.96  44.91  68.22 -36.94 110.25 114.78  31.63  70.25  46.69  71.36\n  20.53  45.51  59.91 115.92   1.72  84.41]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:49<00:00, 21.86s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 91.6514\nTrain Loss: 0.0003, Train MAE: 99.6072\nVal Loss: 0.0003, Val MAE: 91.6514\nLearning Rate: 0.001000\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:22<10:14, 34.13s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [155.  58.  55. 125.  90. 118. 146. 230.  53.  67.  53. 111.  76.  56.\n  36. 166.]\n  Batch 1 (Train): Predicted Counts (from density map): [135.84 151.62  62.53 161.78  23.37 -28.29 -30.66 136.69  63.     8.48\n  84.37 151.84  75.4  121.26  61.64 119.4 ]\n  Batch 2 (Train): Actual Counts (GT from .mat): [ 50.  55.  54.  70.  46.  13.  33.  41.  34. 114.  79. 102. 271. 175.\n  30. 156.]\n  Batch 2 (Train): Predicted Counts (from density map): [157.49  87.31  88.77  78.73 144.7  101.14 -11.01  15.56  82.89   2.78\n  63.55  94.04 141.75 111.28  57.2   90.66]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:22<05:16, 18.60s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [283. 229. 148. 201.  28.  39.  30.  29. 136.  86. 344. 125. 106.  75.\n  50.  27.]\n  Batch 3 (Train): Predicted Counts (from density map): [ 17.87  -7.74  65.41  35.9   58.    96.73 107.05 -15.9   17.48  61.09\n  81.68  47.31  65.54 100.51  40.03  94.41]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:20<00:00, 19.02s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.17s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [120.56 111.45 171.14  -5.2   58.64 143.32 140.5  137.39 135.61 105.89\n   3.91  89.23  30.09  38.8   27.02  95.42]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [129.31  37.16  70.7  139.42 114.82  88.4  184.47 118.27 103.81  52.11\n 144.63  19.9   44.33 112.78 -42.57 103.19]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 70.91  51.96  79.25 -17.72 113.12 109.16  41.5   78.47  61.22  82.95\n  23.98  63.04  69.48 123.73  18.7   89.55]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 85.6936\nTrain Loss: 0.0003, Train MAE: 87.2734\nVal Loss: 0.0003, Val MAE: 85.6936\nLearning Rate: 0.001000\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|         | 2/20 [01:20<09:55, 33.06s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [ 16.  50.  22. 134.  89. 380.  49. 136. 154. 355.  26. 108. 222.  31.\n 122.  74.]\n  Batch 1 (Train): Predicted Counts (from density map): [ 55.33 176.58  87.32 100.7   66.69 172.32  68.94  98.88  46.73  88.39\n 172.61 137.98  23.92 186.47 131.99  48.1 ]\n  Batch 2 (Train): Actual Counts (GT from .mat): [ 42. 152.  89. 276.  54.  28. 123. 139.  86. 143.  67.  99.  43.  23.\n  39.  27.]\n  Batch 2 (Train): Predicted Counts (from density map): [ 71.34  27.21 130.36  44.47   7.46  78.8   30.48  16.82 102.54  -7.53\n -46.03 -19.13   4.68  -2.63 134.35 -14.63]\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|        | 4/20 [01:20<02:55, 10.97s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 3 (Train): Actual Counts (GT from .mat): [175. 121.  42.  38.  43. 109.  43.  69.  97. 257. 269. 100.  31.  27.\n  84.  46.]\n  Batch 3 (Train): Predicted Counts (from density map): [ 94.13  97.32 -14.69 130.5  -52.12  87.38  80.69 105.75  53.08  55.77\n  79.56  26.68 114.62  25.68  92.6  150.21]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:16<00:00, 18.83s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.20s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [130.29 122.38 178.09  10.26  76.9  133.43 146.68 146.8  139.43 114.97\n  17.8   97.74  56.95  45.81  35.17  97.75]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [137.75  52.45  72.66 149.84 125.33  96.17 181.61 121.69 115.8   49.4\n 156.43  30.46  58.27 118.7  -22.89 114.55]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 75.32  56.62  93.78  -5.21 122.98 117.23  58.65  90.59  72.82  76.18\n  37.03  74.85  79.51 129.38  30.86 101.07]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 80.4978\nTrain Loss: 0.0002, Train MAE: 85.3549\nVal Loss: 0.0003, Val MAE: 80.4978\nLearning Rate: 0.001000\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   5%|         | 1/20 [01:09<21:56, 69.31s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Train): Actual Counts (GT from .mat): [269.  47. 107. 139. 282. 101. 123.  89. 229. 220. 283. 125.  27. 267.\n 143. 276.]\n  Batch 1 (Train): Predicted Counts (from density map): [ 22.43 -46.64  97.81  44.55 145.72 124.8   50.9   40.91 102.4   98.33\n  73.91  39.6  -34.51 115.14 123.05  64.01]\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|        | 3/20 [01:16<05:03, 17.85s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 2 (Train): Actual Counts (GT from .mat): [ 74.  28.  43.  78. 257. 355.  30. 293. 167.  38.  68. 102.  87.  63.\n 178.  90.]\n  Batch 2 (Train): Predicted Counts (from density map): [-63.83  63.43 -22.01  39.76  30.53  -4.26   0.34 -30.33  69.32  53.57\n  -9.83 -34.21 161.83  44.57 -70.36  34.02]\n  Batch 3 (Train): Actual Counts (GT from .mat): [ 38.  53.  79.  70.  89.  54.  86. 283.  50. 239.  47. 234. 123.  58.\n  40.  27.]\n  Batch 3 (Train): Predicted Counts (from density map): [-29.63  98.79 102.35  50.32 149.    90.24  74.44  92.67 132.83  96.05\n  67.77  75.9   58.11 114.61  12.3   45.05]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|| 20/20 [06:24<00:00, 19.21s/it]\nValidation:  80%|  | 4/5 [01:14<00:14, 14.14s/it]","output_type":"stream"},{"name":"stdout","text":"  Batch 1 (Val): Actual Counts (GT from .mat): [ 36.  96. 117. 157. 104. 146. 148.  81. 152. 205. 161. 195. 247.  23.\n  33.  47.]\n  Batch 1 (Val): Predicted Counts (from density map): [137.23 132.33 184.71  22.11  95.37 124.03 154.91 154.54 146.36 120.67\n  29.34 104.54  87.49  48.51  41.18 100.23]\n  Batch 2 (Val): Actual Counts (GT from .mat): [252. 118.  84. 304. 227.  81.  72.  34. 153.  12. 318.  23.  61. 191.\n  81. 144.]\n  Batch 2 (Val): Predicted Counts (from density map): [146.01  65.94  74.8  161.4  135.17 102.14 180.77 122.93 128.27  39.95\n 166.86  37.94  72.96 121.89  -9.95 126.03]\n  Batch 3 (Val): Actual Counts (GT from .mat): [ 94.  47. 261. 150.  32.  90. 169.  62. 243.  49. 111. 167. 151. 146.\n 143.  85.]\n  Batch 3 (Val): Predicted Counts (from density map): [ 82.04  60.15 107.95   7.93 130.01 122.8   77.12 100.14  83.78  68.27\n  49.78  85.74  89.24 133.04  37.8  112.03]\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|| 5/5 [01:44<00:00, 20.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"New best model saved with MAE: 76.0274\nTrain Loss: 0.0002, Train MAE: 74.9584\nVal Loss: 0.0002, Val MAE: 76.0274\nLearning Rate: 0.001000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nStarting evaluation on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|| 20/20 [06:12<00:00, 18.62s/it]","output_type":"stream"},{"name":"stdout","text":"\nTest Results:\nMAE: 73.8225\nMSE: 10445.7630\nRMSE: 102.2045\nFinal Test MAE: 73.8225\nFinal Test MSE: 10445.7630\nFinal Test RMSE: 102.2045\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}