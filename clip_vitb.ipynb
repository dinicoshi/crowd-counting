{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e128edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import clip\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c06951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBCHead(nn.Module):\n",
    "    def _init_(self, input_dim, num_bins=100, reduction=8):\n",
    "        super(EBCHead, self)._init_()\n",
    "        self.num_bins = num_bins\n",
    "        self.reduction = reduction \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_bins, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=1),\n",
    "            nn.ReLU(inplace=True) \n",
    "        )\n",
    "\n",
    "        self.register_buffer('bin_centers', torch.arange(0, num_bins, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_logits = self.classifier(x)\n",
    "        cls_probs = F.softmax(cls_logits, dim=1)\n",
    "        density_map = self.regressor(x)\n",
    "        count_map = torch.sum(cls_probs * self.bin_centers.view(1, -1, 1, 1), dim=1, keepdim=True)\n",
    "\n",
    "        return {\n",
    "            'cls_logits': cls_logits,\n",
    "            'cls_probs': cls_probs,\n",
    "            'density_map': density_map,\n",
    "            'count_map': count_map\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7810b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder(nn.Module):\n",
    "    def _init_(self, model_name='ViT-B/16', freeze_clip=True, input_size=384):\n",
    "        super(CLIPEncoder, self)._init_()\n",
    "        self.input_size = input_size\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.clip_model, self.preprocess = clip.load(model_name, device=device)\n",
    "\n",
    "        if hasattr(self.clip_model.visual, 'conv1') and hasattr(self.clip_model.visual.conv1, 'weight'):\n",
    "            self.clip_conv1_dtype = self.clip_model.visual.conv1.weight.dtype\n",
    "        else:\n",
    "            self.clip_conv1_dtype = torch.float32\n",
    "\n",
    "        if freeze_clip:\n",
    "            for param in self.clip_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.freeze_clip = freeze_clip\n",
    "\n",
    "        visual_encoder_type = self.clip_model.visual._class.name_\n",
    "\n",
    "        if 'VisionTransformer' in visual_encoder_type or 'ViT' in visual_encoder_type:\n",
    "            self.clip_patch_size = self.clip_model.visual.conv1.kernel_size[0]\n",
    "            if model_name == 'ViT-B/16':\n",
    "                self.feature_dim = 768\n",
    "            else:\n",
    "                self.feature_dim = self.clip_model.visual.output_dim\n",
    "\n",
    "            if hasattr(self.clip_model.visual, 'positional_embedding'):\n",
    "                original_pos_embed = self.clip_model.visual.positional_embedding.float()\n",
    "                original_seq_len = original_pos_embed.shape[0]\n",
    "                target_seq_len = (input_size // self.clip_patch_size) * (input_size // self.clip_patch_size) + 1\n",
    "\n",
    "                if original_seq_len != target_seq_len:\n",
    "                    cls_pos_embed = original_pos_embed[:1, :]\n",
    "                    patch_pos_embed = original_pos_embed[1:, :]\n",
    "\n",
    "                    original_grid_size = int(math.sqrt(patch_pos_embed.shape[0]))\n",
    "                    patch_pos_embed = patch_pos_embed.view(original_grid_size, original_grid_size, -1)\n",
    "                    patch_pos_embed = patch_pos_embed.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "                    new_grid_size = input_size // self.clip_patch_size\n",
    "                    interpolated_patch_pos_embed = F.interpolate(patch_pos_embed,\n",
    "                                                                 size=(new_grid_size, new_grid_size),\n",
    "                                                                 mode='bicubic',\n",
    "                                                                 align_corners=False)\n",
    "\n",
    "                    interpolated_patch_pos_embed = interpolated_patch_pos_embed.squeeze(0).flatten(1, 2).permute(1, 0)\n",
    "                    new_pos_embed = torch.cat([cls_pos_embed, interpolated_patch_pos_embed], dim=0)\n",
    "\n",
    "                    self.clip_model.visual.positional_embedding = nn.Parameter(new_pos_embed.to(self.clip_conv1_dtype).to(device))\n",
    "                else:\n",
    "                    self.clip_model.visual.positional_embedding = nn.Parameter(self.clip_model.visual.positional_embedding.to(device).to(self.clip_conv1_dtype))\n",
    "\n",
    "        elif 'ResNet' in visual_encoder_type or 'RN' in visual_encoder_type:\n",
    "            self.clip_patch_size = 32\n",
    "            if hasattr(self.clip_model.visual, 'attnpool'):\n",
    "                raise NotImplementedError(f\"CLIPEncoder does not currently support ResNet models with attention pooling like {model_name} for spatial features.\")\n",
    "            elif hasattr(self.clip_model.visual, 'layer4'):\n",
    "                if hasattr(self.clip_model.visual.layer4[-1], 'conv3'):\n",
    "                    self.feature_dim = self.clip_model.visual.layer4[-1].conv3.out_channels\n",
    "                elif hasattr(self.clip_model.visual.layer4[-1], 'conv2'):\n",
    "                    self.feature_dim = self.clip_model.visual.layer4[-1].conv2.out_channels\n",
    "                else:\n",
    "                    raise AttributeError(\"Could not find output channels in CLIP ResNet layer4.\")\n",
    "            else:\n",
    "                raise NotImplementedError(f\"CLIPEncoder does not currently support ResNet models like {model_name} for spatial features.\")\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported CLIP visual encoder type: {visual_encoder_type}\")\n",
    "\n",
    "        self.output_spatial_size = input_size // self.clip_patch_size\n",
    "        self.adapter_conv1 = nn.Conv2d(self.feature_dim, 512, kernel_size=3, padding=1).to(device)\n",
    "        self.adapter_relu1 = nn.ReLU(inplace=True).to(device)\n",
    "        self.adapter_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1).to(device)\n",
    "        self.adapter_relu2 = nn.ReLU(inplace=True).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        visual_transformer = self.clip_model.visual\n",
    "        current_device = x.device\n",
    "\n",
    "        with torch.no_grad() if self.freeze_clip else torch.enable_grad():\n",
    "            visual_encoder_type = visual_transformer._class.name_\n",
    "\n",
    "            if 'VisionTransformer' in visual_encoder_type:\n",
    "                x = x.to(visual_transformer.conv1.weight.dtype)\n",
    "                x = visual_transformer.conv1(x)\n",
    "                x = x.flatten(2).permute(0, 2, 1)\n",
    "                class_embedding = visual_transformer.class_embedding.to(x.dtype).to(current_device)\n",
    "                pos_embedding = visual_transformer.positional_embedding.to(x.dtype).to(current_device)\n",
    "                x = torch.cat([class_embedding.unsqueeze(0).expand(x.shape[0], -1, -1), x], dim=1)\n",
    "                x = x + pos_embedding.unsqueeze(0)\n",
    "                x = visual_transformer.ln_pre(x)\n",
    "                for resblock in visual_transformer.transformer.resblocks:\n",
    "                    x = resblock(x)\n",
    "                if hasattr(visual_transformer, 'ln_post'):\n",
    "                    x = visual_transformer.ln_post(x)\n",
    "\n",
    "                spatial_features_flat = x[:, 1:, :]\n",
    "                B, N_patches, D = spatial_features_flat.shape\n",
    "                H = W = int(math.sqrt(N_patches))\n",
    "                if H * W != N_patches:\n",
    "                    raise ValueError(f\"Could not reshape {N_patches} patches into a square grid. Input size might not be suitable.\")\n",
    "                spatial_features = spatial_features_flat.permute(0, 2, 1).view(B, D, H, W)\n",
    "\n",
    "            elif 'ResNet' in visual_encoder_type:\n",
    "                x_rn = x.to(visual_transformer.conv1.weight.dtype)\n",
    "\n",
    "                try:\n",
    "                    x_rn = visual_transformer.conv1(x_rn)\n",
    "                    x_rn = visual_transformer.bn1(x_rn)\n",
    "                    x_rn = visual_transformer.relu(x_rn)\n",
    "                    x_rn = visual_transformer.avgpool(x_rn)\n",
    "                    x_rn = visual_transformer.layer1(x_rn)\n",
    "                    x_rn = visual_transformer.layer2(x_rn)\n",
    "                    x_rn = visual_transformer.layer3(x_rn)\n",
    "                    spatial_features = visual_transformer.layer4(x_rn)\n",
    "\n",
    "                    _, _, h_feat, w_feat = spatial_features.shape\n",
    "                    expected_h = expected_w = self.output_spatial_size\n",
    "\n",
    "                    if h_feat != expected_h or w_feat != expected_w:\n",
    "                        print(f\"Warning: ResNet spatial feature size mismatch. Expected {expected_h}x{expected_w}, got {h_feat}x{w_feat}.\")\n",
    "\n",
    "                    if spatial_features.shape[1] != self.feature_dim:\n",
    "                        print(f\"Warning: ResNet feature dimension mismatch. Expected {self.feature_dim}, got {spatial_features.shape[1]}.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise NotImplementedError(f\"Failed to extract spatial features from CLIP ResNet model: {model_name}.\")\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Unsupported CLIP visual encoder type in forward pass: {visual_encoder_type}\")\n",
    "\n",
    "        features = spatial_features.float()\n",
    "        features = self.adapter_conv1(features)\n",
    "        features = self.adapter_relu1(features)\n",
    "\n",
    "        if features.shape[1] != 512:\n",
    "            raise ValueError(f\"Channel mismatch: Expected 512 channels after adapter_conv1 and relu1, but got {features.shape[1]}.\")\n",
    "\n",
    "        features = self.adapter_conv2(features)\n",
    "        features = self.adapter_relu2(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEBC(nn.Module):\n",
    "    def __init__(self, clip_model='ViT-B/16', num_bins=100, reduction=8, freeze_clip=True, input_size=384):\n",
    "        super(CLIPEBC, self).__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.encoder = CLIPEncoder(clip_model, freeze_clip, input_size=input_size)\n",
    "        self.actual_encoder_output_spatial_size = input_size // self.encoder.clip_patch_size\n",
    "        self.model_output_size = input_size // reduction\n",
    "\n",
    "        if self.model_output_size != self.actual_encoder_output_spatial_size:\n",
    "            print(f\"WARNING (CLIPEBC init): Dataset target GT size ({self.model_output_size}x{self.model_output_size}) based on reduction={self.reduction}\")\n",
    "            print(f\"         does NOT match actual encoder output size ({self.actual_encoder_output_spatial_size}x{self.actual_encoder_output_spatial_size}) based on input_size={self.input_size} and CLIP patch_size={self.encoder.clip_patch_size}.\")\n",
    "            print(f\"         The 'run_clip_ebc_pipeline' should have corrected the 'reduction' parameter to {self.encoder.clip_patch_size}.\")\n",
    "\n",
    "        self.ebc_head = EBCHead(512, num_bins, reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        outputs = self.ebc_head(features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMCountLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=1.0, gamma=1.0, num_bins=100):\n",
    "        super(DMCountLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')\n",
    "        self.l1_loss = nn.L1Loss(reduction='sum')\n",
    "\n",
    "        self.register_buffer('bin_centers', torch.arange(0, num_bins, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, pred_outputs, gt_density_map, gt_total_count, gt_block_counts):\n",
    "        pred_density = pred_outputs['density_map']\n",
    "        pred_count_map = pred_outputs['count_map']\n",
    "\n",
    "        pred_total_count = pred_count_map.sum(dim=(2, 3)).squeeze()\n",
    "        if pred_total_count.ndim == 0:\n",
    "            pred_total_count = pred_total_count.unsqueeze(0)\n",
    "\n",
    "        total_count_loss = self.l1_loss(pred_total_count, gt_total_count.float()) / pred_total_count.shape[0]\n",
    "        gt_block_counts_float = gt_block_counts.float()\n",
    "        block_count_loss = self.l1_loss(pred_count_map.squeeze(1), gt_block_counts_float) / pred_count_map.shape[0]\n",
    "\n",
    "        if pred_density.dtype != gt_density_map.dtype:\n",
    "            gt_density_map = gt_density_map.to(pred_density.dtype)\n",
    "\n",
    "        if pred_density.shape[-2:] != gt_density_map.shape[-2:]:\n",
    "            raise ValueError(\"Spatial shape mismatch between predicted and ground truth density maps. Adjust 'reduction' in pipeline configuration.\")\n",
    "\n",
    "        density_loss = self.mse_loss(pred_density, gt_density_map) / pred_density.shape[0]\n",
    "\n",
    "        total_loss = self.alpha * density_loss + \\\n",
    "                     self.beta * total_count_loss + \\\n",
    "                     self.gamma * block_count_loss\n",
    "\n",
    "        return total_loss, density_loss, total_count_loss, block_count_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86814274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, images_dir, gt_dir, input_size=384, is_train=True, reduction=8, num_bins=100):\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.input_size = input_size\n",
    "        self.is_train = is_train\n",
    "        self.reduction = reduction\n",
    "        self.model_output_size = self.input_size // self.reduction\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "        self.image_files = sorted(glob.glob(os.path.join(images_dir, '*')))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if is_train:\n",
    "            self.transform_aug = transforms.Compose([\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform_aug = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        gt_name = os.path.basename(img_path).replace('.jpg', '.mat').replace('.png', '.mat')\n",
    "        gt_path = os.path.join(self.gt_dir, gt_name)\n",
    "\n",
    "        points = np.array([]).reshape(0, 2)\n",
    "        if os.path.exists(gt_path):\n",
    "            try:\n",
    "                gt_data = sio.loadmat(gt_path)\n",
    "                if 'image_info' in gt_data:\n",
    "                    points = gt_data['image_info'][0,0][0,0][0]\n",
    "                elif 'annPoints' in gt_data:\n",
    "                    points = gt_data['annPoints']\n",
    "                else:\n",
    "                    found = False\n",
    "                    for key in gt_data.keys():\n",
    "                        if not key.startswith(''):\n",
    "                            temp_points = gt_data[key]\n",
    "                            if len(temp_points.shape) == 2 and temp_points.shape[1] >= 2:\n",
    "                                points = temp_points[:, :2]\n",
    "                                found = True\n",
    "                                break\n",
    "                    if not found:\n",
    "                        points = np.array([]).reshape(0, 2)\n",
    "            except Exception:\n",
    "                points = np.array([]).reshape(0, 2)\n",
    "\n",
    "        density_map_orig_res = self.create_density_map(points, (original_width, original_height))\n",
    "\n",
    "        gt_block_counts = np.zeros((self.model_output_size, self.model_output_size), dtype=np.int32)\n",
    "        if len(points) > 0:\n",
    "            scaled_points_x = points[:, 0] * (self.model_output_size / original_width)\n",
    "            scaled_points_y = points[:, 1] * (self.model_output_size / original_height)\n",
    "            for i in range(len(scaled_points_x)):\n",
    "                block_x = int(np.clip(scaled_points_x[i], 0, self.model_output_size - 1))\n",
    "                block_y = int(np.clip(scaled_points_y[i], 0, self.model_output_size - 1))\n",
    "                gt_block_counts[block_y, block_x] += 1\n",
    "\n",
    "        gt_block_counts_tensor = torch.from_numpy(gt_block_counts).long()\n",
    "\n",
    "        if self.is_train and self.transform_aug:\n",
    "            image = self.transform_aug(image)\n",
    "\n",
    "        image_tensor = self.transform(image)\n",
    "\n",
    "        density_map_tensor_orig = torch.from_numpy(density_map_orig_res).float().unsqueeze(0)\n",
    "        original_sum_density = density_map_tensor_orig.sum().item()\n",
    "\n",
    "        density_map_resized = F.interpolate(density_map_tensor_orig.unsqueeze(0),\n",
    "                                            size=(self.model_output_size, self.model_output_size),\n",
    "                                            mode='bilinear',\n",
    "                                            align_corners=False).squeeze(0)\n",
    "\n",
    "        if original_sum_density > 0:\n",
    "            current_sum_density = density_map_resized.sum().item()\n",
    "            if current_sum_density > 1e-6:\n",
    "                density_map_resized = density_map_resized * (original_sum_density / current_sum_density)\n",
    "            else:\n",
    "                density_map_resized = torch.zeros_like(density_map_resized)\n",
    "        elif density_map_resized.sum().item() > 1e-6:\n",
    "            density_map_resized = torch.zeros_like(density_map_resized)\n",
    "\n",
    "        gt_total_count = torch.tensor(len(points), dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'density_map': density_map_resized,\n",
    "            'gt_count': gt_total_count,\n",
    "            'gt_block_counts': gt_block_counts_tensor,\n",
    "            'points': points\n",
    "        }\n",
    "\n",
    "    def create_density_map(self, points, img_size):\n",
    "        width, height = img_size\n",
    "        density_map = np.zeros((height, width), dtype=np.float32)\n",
    "        if len(points) == 0:\n",
    "            return density_map\n",
    "        sigma = max(1, min(width, height) // 80)\n",
    "        for point in points:\n",
    "            x, y = int(round(point[0])), int(round(point[1]))\n",
    "            if 0 <= x < width and 0 <= y < height:\n",
    "                kernel_radius = int(round(3 * sigma))\n",
    "                kernel_size = 2 * kernel_radius + 1\n",
    "                gaussian_kernel = np.outer(\n",
    "                    cv2.getGaussianKernel(kernel_size, sigma),\n",
    "                    cv2.getGaussianKernel(kernel_size, sigma)\n",
    "                )\n",
    "                x_start_map = max(0, x - kernel_radius)\n",
    "                y_start_map = max(0, y - kernel_radius)\n",
    "                x_end_map = min(width, x + kernel_radius + 1)\n",
    "                y_end_map = min(height, y + kernel_radius + 1)\n",
    "                x_start_kernel = kernel_radius - (x - x_start_map)\n",
    "                y_start_kernel = kernel_radius - (y - y_start_map)\n",
    "                x_end_kernel = kernel_radius + (x_end_map - x)\n",
    "                y_end_kernel = kernel_radius + (y_end_map - y)\n",
    "                density_map[y_start_map:y_end_map, x_start_map:x_end_map] += \\\n",
    "                    gaussian_kernel[y_start_kernel:y_end_kernel, x_start_kernel:x_end_kernel]\n",
    "        current_sum = density_map.sum()\n",
    "        if current_sum > 1e-6:\n",
    "            if len(points) > 0:\n",
    "                density_map = density_map / (current_sum / len(points))\n",
    "        return density_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71935ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=1e-4, device='cuda'):\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = DMCountLoss(alpha=1.0, beta=1.0, gamma=1.0, num_bins=model.ebc_head.num_bins)\n",
    "\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    best_mae = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_maes = []\n",
    "\n",
    "    actual_model_output_spatial_size = model.actual_encoder_output_spatial_size\n",
    "    print(f\"Model's actual spatial output size from encoder: {actual_model_output_spatial_size}x{actual_model_output_spatial_size}\")\n",
    "    dataset_target_output_size = train_loader.dataset.model_output_size\n",
    "    if dataset_target_output_size != actual_model_output_spatial_size:\n",
    "         print(f\"ERROR: Dataset expects GT at {dataset_target_output_size}x{dataset_target_output_size}, but model outputs {actual_model_output_spatial_size}x{actual_model_output_spatial_size}.\")\n",
    "         print(\"         This indicates a mismatch in the 'reduction' parameter. The run_clip_ebc_pipeline should have already corrected this.\")\n",
    "         raise ValueError(f\"Dataset/Model spatial size mismatch. Dataset target: {dataset_target_output_size}, Model actual: {actual_model_output_spatial_size}. Check 'reduction' parameter setup.\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_density_loss_sum = 0.0\n",
    "        train_total_count_loss_sum = 0.0\n",
    "        train_block_count_loss_sum = 0.0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n",
    "\n",
    "        for batch in train_pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            gt_density_map = batch['density_map'].to(device).float()\n",
    "            gt_total_counts = batch['gt_count'].to(device).float()\n",
    "            gt_block_counts = batch['gt_block_counts'].to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            current_total_loss, density_loss, total_count_loss, block_count_loss = criterion(\n",
    "                outputs, gt_density_map, gt_total_counts, gt_block_counts\n",
    "            )\n",
    "\n",
    "            current_total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += current_total_loss.item()\n",
    "            train_density_loss_sum += density_loss.item()\n",
    "            train_total_count_loss_sum += total_count_loss.item()\n",
    "            train_block_count_loss_sum += block_count_loss.item()\n",
    "\n",
    "            train_pbar.set_postfix({\n",
    "                'T_Loss': f'{current_total_loss.item():.4f}',\n",
    "                'D_Loss': f'{density_loss.item():.4f}',\n",
    "                'C_Loss': f'{total_count_loss.item():.4f}',\n",
    "                'B_Loss': f'{block_count_loss.item():.4f}'\n",
    "            })\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_train_total_loss = total_train_loss / len(train_loader)\n",
    "        avg_train_density_loss = train_density_loss_sum / len(train_loader)\n",
    "        avg_train_total_count_loss = train_total_count_loss_sum / len(train_loader)\n",
    "        avg_train_block_count_loss = train_block_count_loss_sum / len(train_loader)\n",
    "\n",
    "        train_losses.append(avg_train_total_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_mae = 0.0\n",
    "        val_rmse = 0.0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "\n",
    "            for batch in val_pbar:\n",
    "                images = batch['image'].to(device)\n",
    "                gt_counts_batch = batch['gt_count'].cpu().numpy()\n",
    "\n",
    "                outputs = model(images)\n",
    "                pred_counts_batch = outputs['count_map'].sum(dim=(2, 3)).squeeze().cpu().numpy()\n",
    "\n",
    "                if pred_counts_batch.ndim == 0:\n",
    "                    pred_counts_batch = np.array([pred_counts_batch.item()])\n",
    "                if gt_counts_batch.ndim == 0:\n",
    "                    gt_counts_batch = np.array([gt_counts_batch.item()])\n",
    "\n",
    "                mae_batch = np.mean(np.abs(pred_counts_batch - gt_counts_batch))\n",
    "                rmse_batch = np.sqrt(np.mean((pred_counts_batch - gt_counts_batch) ** 2))\n",
    "\n",
    "                val_mae += mae_batch * len(gt_counts_batch)\n",
    "                val_rmse += rmse_batch * len(gt_counts_batch)\n",
    "                val_samples += len(gt_counts_batch)\n",
    "\n",
    "                val_pbar.set_postfix({'MAE': f'{mae_batch:.2f}'})\n",
    "\n",
    "        avg_val_mae = val_mae / val_samples\n",
    "        avg_val_rmse = val_rmse / val_samples\n",
    "\n",
    "        val_maes.append(avg_val_mae)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Losses: Total={avg_train_total_loss:.4f}, Density={avg_train_density_loss:.4f}, '\\\n",
    "              f'TotalCount={avg_train_total_count_loss:.4f}, BlockCount={avg_train_block_count_loss:.4f}')\n",
    "        print(f'  Val MAE: {avg_val_mae:.2f}')\n",
    "        print(f'  Val RMSE: {avg_val_rmse:.2f}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        if avg_val_mae < best_mae:\n",
    "            best_mae = avg_val_mae\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f'New best MAE: {best_mae:.2f} - Model state saved.')\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f'Loaded best model state with MAE: {best_mae:.2f}')\n",
    "\n",
    "    return model, train_losses, val_maes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_pred_counts = []\n",
    "    all_gt_counts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc='Testing')\n",
    "\n",
    "        for batch in test_pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            gt_counts_batch = batch['gt_count'].cpu().numpy()\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred_counts_batch = outputs['count_map'].sum(dim=(2, 3)).squeeze().cpu().numpy()\n",
    "\n",
    "            if pred_counts_batch.ndim == 0:\n",
    "                pred_counts_batch = np.array([pred_counts_batch.item()])\n",
    "            if gt_counts_batch.ndim == 0:\n",
    "                gt_counts_batch = np.array([gt_counts_batch.item()])\n",
    "\n",
    "            all_pred_counts.extend(pred_counts_batch)\n",
    "            all_gt_counts.extend(gt_counts_batch)\n",
    "\n",
    "    mae = mean_absolute_error(all_gt_counts, all_pred_counts)\n",
    "    rmse = np.sqrt(mean_squared_error(all_gt_counts, all_pred_counts))\n",
    "\n",
    "    print(f'--- Test Results ---')\n",
    "    print(f'Final MAE: {mae:.2f}')\n",
    "    print(f'Final RMSE: {rmse:.2f}')\n",
    "\n",
    "    return mae, rmse, all_pred_counts, all_gt_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, dataset, device='cuda', num_samples=5):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nVisualizing {min(num_samples, len(dataset))} sample results...\")\n",
    "    visual_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    for i, batch in enumerate(visual_loader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        image_tensor = batch['image'].to(device)\n",
    "        gt_density = batch['density_map'].squeeze().cpu().numpy()\n",
    "        gt_count = batch['gt_count'].item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "            pred_density = outputs['density_map'].squeeze().cpu().numpy()\n",
    "            pred_count = outputs['count_map'].sum().item()\n",
    "\n",
    "        img_denorm = image_tensor.squeeze(0).cpu()\n",
    "        img_denorm = inv_normalize(img_denorm)\n",
    "        img_denorm = torch.clamp(img_denorm, 0, 1)\n",
    "\n",
    "        axes[i, 0].imshow(img_denorm.permute(1, 2, 0))\n",
    "        axes[i, 0].set_title(f'Original (GT: {gt_count:.0f})')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(gt_density, cmap='jet')\n",
    "        axes[i, 1].set_title(f'GT Density (Sum: {gt_density.sum():.0f})')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(pred_density, cmap='jet')\n",
    "        axes[i, 2].set_title(f'Pred Density (Sum: {pred_density.sum():.1f}, Count: {pred_count:.1f})')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clip_ebc_pipeline(train_images_dir, train_gt_dir, test_images_dir, test_gt_dir,\n",
    "                          input_size=384, batch_size=8, num_epochs=50, lr=1e-4,\n",
    "                          clip_model_name='ViT-B/16', num_bins=100, freeze_clip=False):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    print(\"Determining CLIP encoder output size and correct reduction factor...\")\n",
    "    try:\n",
    "        dummy_encoder = CLIPEncoder(clip_model_name, freeze_clip=True, input_size=input_size)\n",
    "        actual_encoder_spatial_output_size = input_size // dummy_encoder.clip_patch_size\n",
    "        print(f\"CLIP model '{clip_model_name}' with input size {input_size} has patch size {dummy_encoder.clip_patch_size}, resulting in an actual spatial output size of {actual_encoder_spatial_output_size}x{actual_encoder_spatial_output_size}.\")\n",
    "        correct_reduction = dummy_encoder.clip_patch_size\n",
    "        print(f\"Setting pipeline's 'reduction' parameter to {correct_reduction} to ensure spatial alignment between dataset GT and model output.\")\n",
    "        reduction_for_pipeline = correct_reduction\n",
    "        del dummy_encoder\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    except NotImplementedError as e:\n",
    "        print(f\"Error determining CLIP encoder output size: {e}\")\n",
    "        print(\"Cannot proceed as dataset GT size cannot be aligned with model output size.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while determining CLIP encoder output size: {e}\")\n",
    "        print(\"Cannot proceed.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print('Creating datasets...')\n",
    "    train_dataset = CrowdDataset(train_images_dir, train_gt_dir, input_size,\n",
    "                                 is_train=True, reduction=reduction_for_pipeline, num_bins=num_bins)\n",
    "    test_dataset = CrowdDataset(test_images_dir, test_gt_dir, input_size,\n",
    "                                is_train=False, reduction=reduction_for_pipeline, num_bins=num_bins)\n",
    "\n",
    "    print(f'Number of training samples: {len(train_dataset)}')\n",
    "    print(f'Number of test/validation samples: {len(test_dataset)}')\n",
    "    print(f\"Dataset GT target size (input_size // reduction): {train_dataset.model_output_size}x{train_dataset.model_output_size}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    print('Creating CLIP-EBC model...')\n",
    "    model = CLIPEBC(clip_model=clip_model_name, num_bins=num_bins, reduction=reduction_for_pipeline,\n",
    "                    freeze_clip=freeze_clip, input_size=input_size)\n",
    "\n",
    "    if model.actual_encoder_output_spatial_size != train_dataset.model_output_size:\n",
    "        print(f\"FATAL ERROR: Model's actual encoder output size ({model.actual_encoder_output_spatial_size}) still doesn't match Dataset target size ({train_dataset.model_output_size}).\")\n",
    "        print(\"There is likely a deep-seated issue in how the model's output size is determined or a fundamental incompatibility.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"Model successfully created. Actual encoder output spatial size: {model.actual_encoder_output_spatial_size}x{model.actual_encoder_output_spatial_size}\")\n",
    "\n",
    "    print('Starting training...')\n",
    "    model, train_losses, val_maes = train_model(\n",
    "        model, train_loader, test_loader, num_epochs=num_epochs, lr=lr, device=device\n",
    "    )\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Training did not complete successfully.\")\n",
    "        return None, None, None\n",
    "\n",
    "    print('\\nTesting model on the full test set...')\n",
    "    mae, rmse, _, _ = test_model(model, test_loader, device)\n",
    "\n",
    "    print('\\nVisualizing sample predictions...')\n",
    "    visualize_results(model, test_dataset, device, num_samples=3)\n",
    "\n",
    "    print('\\nPlotting training curves...')\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Total Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_maes)\n",
    "    plt.title('Validation MAE Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcb026",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    TRAIN_IMAGES_DIR = 'crowd_wala_dataset\\\\train_data\\\\images'\n",
    "    TRAIN_GT_DIR = 'crowd_wala_dataset\\\\train_data\\\\ground_truth'\n",
    "    TEST_IMAGES_DIR = 'crowd_wala_dataset\\\\test_data\\\\images'\n",
    "    TEST_GT_DIR = 'crowd_wala_dataset\\\\test_data\\\\ground_truth'\n",
    "\n",
    "    if not all(os.path.exists(d) for d in [TRAIN_IMAGES_DIR, TRAIN_GT_DIR, TEST_IMAGES_DIR, TEST_GT_DIR]):\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"! WARNING: Dataset paths are not set or do not exist.              !\")\n",
    "        print(\"! Please update TRAIN_IMAGES_DIR, TRAIN_GT_DIR, TEST_IMAGES_DIR,   !\")\n",
    "        print(\"! and TEST_GT_DIR in the example usage section with your actual    !\")\n",
    "        print(\"! dataset paths. The pipeline will not run without valid data.     !\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    else:\n",
    "        INPUT_SIZE = 384\n",
    "        BATCH_SIZE = 8\n",
    "        NUM_EPOCHS = 5\n",
    "        LEARNING_RATE = 1e-4\n",
    "        CLIP_MODEL = 'ViT-B/16'\n",
    "        NUM_BINS = 100\n",
    "        FREEZE_CLIP = False\n",
    "\n",
    "        print(f\"Starting CLIP-EBC pipeline with CLIP model: {CLIP_MODEL}, Input Size: {INPUT_SIZE}, Freeze CLIP: {FREEZE_CLIP}\")\n",
    "        trained_model, final_mae, final_rmse = run_clip_ebc_pipeline(\n",
    "            train_images_dir=TRAIN_IMAGES_DIR,\n",
    "            train_gt_dir=TRAIN_GT_DIR,\n",
    "            test_images_dir=TEST_IMAGES_DIR,\n",
    "            test_gt_dir=TEST_GT_DIR,\n",
    "            input_size=INPUT_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            lr=LEARNING_RATE,\n",
    "            clip_model_name=CLIP_MODEL,\n",
    "            num_bins=NUM_BINS,\n",
    "            freeze_clip=FREEZE_CLIP\n",
    "        )\n",
    "\n",
    "        if trained_model is not None:\n",
    "            print(f'\\nPipeline Finished: Final Test MAE: {final_mae:.2f}, Final Test RMSE: {final_rmse:.2f}')\n",
    "        else:\n",
    "            print(\"\\nPipeline aborted due to configuration or runtime issues.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
